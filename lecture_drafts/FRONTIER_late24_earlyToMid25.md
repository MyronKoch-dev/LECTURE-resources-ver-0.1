# Frontier AI and Adjacent Technologies (2024–2025): A Comprehensive Overview





*(**Modalities legend**: *Text* = natural language; *Vision* = image input/output; *Video* = video analysis/generation; *Code* = code generation/debugging; *UI* = computer UI control. Bold indicates notable new modality for that model.)  
*(OpenAI's *o1*, *o3* refer to their specialized "observer" series models focused on step-by-step reasoning.)*

### OpenAI GPT-4o, GPT-4.5, and GPT-4 Turbo

**GPT-4o ("Omni")** was OpenAI's 2024 flagship model, effectively an improved GPT-4 with similar intelligence but significantly faster responses (ChatGPT — Release Notes | OpenAI Help Center). It introduced multimodal capabilities (text and image understanding) to all users and laid the groundwork for voice and tools integration (ChatGPT — Release Notes | OpenAI Help Center). Upon launch, GPT-4o matched GPT-4's abilities but with notable enhancements: it followed complex instructions more faithfully, solved harder coding problems with cleaner code, and produced more "intuitive, collaborative" responses (ChatGPT — Release Notes | OpenAI Help Center) (ChatGPT — Release Notes | OpenAI Help Center). In May 2024 it became the default model for ChatGPT Plus/Enterprise, with *5×* higher message limits than the old GPT-4 (ChatGPT — Release Notes | OpenAI Help Center). GPT-4o also expanded context length to 128,000 tokens in the API (allowing ~300 pages of text in one prompt) (GPT-4 Turbo in the OpenAI API | OpenAI Help Center). This huge context window enables working with lengthy documents or transcripts seamlessly. GPT-4o supports function calling and plugins, meaning it can invoke external APIs or run code as part of its answers.

To bring GPT-4-level AI to smaller devices and budgets, OpenAI released **GPT-4o Mini** in mid-2024. Despite a much smaller parameter count, GPT-4o Mini "surpasses GPT-3.5 Turbo and other small models" on academic benchmarks (ChatGPT — Release Notes | OpenAI Help Center). It retains multimodal reasoning (e.g. vision understanding) and strong function-calling ability (ChatGPT — Release Notes | OpenAI Help Center). This model gives developers an affordable option to leverage advanced GPT-4-class performance in their own apps.

**GPT-4 Turbo** (2024) was an enhanced version of GPT-4 focusing on efficiency. It offered the same base knowledge and skill as GPT-4, but at a fraction of the cost (about 3× cheaper inputs) and with the full 128k context window (GPT-4 Turbo in the OpenAI API | OpenAI Help Center) (GPT-4 Turbo in the OpenAI API | OpenAI Help Center). This model was ideal for applications needing long-context understanding (e.g. analyzing lengthy contracts or codebases) without the original GPT-4's latency. GPT-4 Turbo became available via API to all GPT-4 users, making large-context AI more accessible. *(Notably, OpenAI's documentation notes GPT-4 Turbo had an updated knowledge cutoff of April 2023 (GPT-4 Turbo in the OpenAI API | OpenAI Help Center), indicating it was trained on more recent data than the original GPT-4.)*

**ChatGPT-4.5** (Feb 2025) represents OpenAI's most advanced general model to date, bridging the gap before GPT-5. It was introduced as a **ChatGPT Pro** feature and API preview (GPT-4.5 explained: Everything you need to know) (GPT-4.5 explained: Everything you need to know). Architecturally, GPT-4.5 builds on GPT-4o with even more extensive pre-training and fine-tuning. OpenAI kept details sparse, but confirmed it is their largest model so far (GPT-4.5 explained: Everything you need to know). Rather than emphasizing chain-of-thought reasoning, GPT-4.5 leans into unsupervised pretraining scale – it responds based on vast knowledge and pattern recognition, while leaving explicit step-by-step reasoning to specialized models (OpenAI's "o-series") (GPT-4.5 explained: Everything you need to know) (GPT-4.5 explained: Everything you need to know). This makes GPT-4.5 a highly generalist AI: extremely knowledgeable and smooth in conversation, though not as slow-deliberate as the reasoning specialists. In practice, GPT-4.5 produces more **factual and concise answers** than its predecessors – it "hallucinates" (makes up facts) far less (Introducing GPT-4.5 | OpenAI) and knows when to stop and ask if the user needs more, thanks to training for higher emotional intelligence (Introducing GPT-4.5 | OpenAI) (Introducing GPT-4.5 | OpenAI). It can handle nuanced dialogues, offering empathy or advice in an "EQ"-aware manner (Introducing GPT-4.5 | OpenAI). GPT-4.5 also supports **multimodal inputs** like image and file uploads for analysis (Introducing GPT-4.5 | OpenAI), and has built-in browsing/search access to provide up-to-date info (Introducing GPT-4.5 | OpenAI) (a user no longer needs a plugin – GPT-4.5 can fetch information live). It integrates with a *Canvas* tool for brainstorming – able to write and edit in a shared document or whiteboard space (Introducing GPT-4.5 | OpenAI). However, at launch it did **not** enable ChatGPT's voice or vision-output modes (those remained on GPT-4o) (Introducing GPT-4.5 | OpenAI). GPT-4.5 focuses on text and knowledge tasks, leaving multimodal generation to other models for now. Early benchmarks show it **improved creative writing and conversational flow**, feeling more like an expert human tutor or colleague. For instance, GPT-4.5's answers tend to be structured and to-the-point, whereas GPT-4o might be more verbose (Introducing GPT-4.5 | OpenAI) (Introducing GPT-4.5 | OpenAI). OpenAI noted that 4.5 was trained with new reinforcement learning techniques and an "instruction hierarchy" to better follow system rules over user prompts (GPT-4.5 explained: Everything you need to know) (GPT-4.5 explained: Everything you need to know). Overall, GPT-4.5 is the current pinnacle of OpenAI's strategy of scaling up pretraining to make a generally smarter model (Introducing GPT-4.5 | OpenAI). It serves as a robust foundation that can later be augmented with explicit reasoning modules or tool-use, as OpenAI hints that "future models will combine pre-training and reasoning" approaches (Introducing GPT-4.5 | OpenAI).

OpenAI's strategy in 2024–2025 clearly bifurcated into **general-purpose large models (GPT-4.5)** and **specialized "reasoners" (the o-series)**. The **OpenAI o-series** (observer models) like **o1** and **o3** are designed to "think for longer before responding" and use chain-of-thought to tackle complex problems (Introducing OpenAI o3 and o4-mini | OpenAI). For example, OpenAI o3 (released alongside GPT-4o Mini) was described as "our most powerful reasoning model... ideal for complex queries requiring multi-step logic" (Introducing OpenAI o3 and o4-mini | OpenAI). It uses an RL-trained approach to reason through math proofs, code, and scientific problems. These models can use tools and calculators explicitly within their reasoning steps. OpenAI even gave o3 full tool access in ChatGPT (browsing, code execution, etc.) and trained it to decide **when** to invoke a tool (Introducing OpenAI o3 and o4-mini | OpenAI). This effectively makes models like o3 early instances of agentic AI within the chat interface. However, the o-series models are smaller and slower than GPT-4.5, so OpenAI positions them for tasks where meticulous logic is needed, whereas GPT-4.5 is the fast knowledge expert (Introducing GPT-4.5 | OpenAI). The interplay of GPT-4.5 with these reasoning agents is an emerging paradigm (OpenAI suggests future models will unify these capabilities).

### Anthropic Claude 3.5 ("Sonnet") and Beyond

Anthropic's **Claude 3.5 "Sonnet"** (late 2024) marked a significant leap for the Claude series. Claude 2 (July 2023) was already notable for a 100k token context and strong language abilities; Claude 3.5 extends this with *agentic* and coding-oriented features. Nicknamed *"Sonnet"*, this model was tuned for **real-world software engineering tasks and tool use** (Claude 3.7 Sonnet \ Anthropic). It can write complex code, debug, and even control computer interfaces in a limited way (Anthropic demonstrated Claude Sonnet operating a virtual computer environment – termed "computer use" – to carry out multi-step tasks) (Claude 3.7 Sonnet \ Anthropic) (Claude 3.7 Sonnet \ Anthropic). Sonnet introduced **Conversational Speech Model integration**, meaning it can produce or understand speech as part of its API (e.g. for voice assistants), and it can handle **multimodal input** to some extent (e.g. interpreting an image or diagram if provided in a prompt, though Anthropic's emphasis remained on text and code). Claude 3.5's standout feature is "*agentic capabilities*": it doesn't just follow a single prompt, but can autonomously break down a goal into sub-tasks, invoke tools or APIs (Anthropic partners integrated Claude into systems like Slack with plugins), and carry on long-running sessions with persistent memory. Anthropic reported that Claude 3.5 Sonnet can even use a web browser and keyboard – for example, one demo had Claude Sonnet taking actions in a simulated browser to research and gather information for the user. This blurs the line between a static chatbot and a proactive *AI assistant*.

Technically, Claude 3.5 introduced a **hybrid architecture** under the hood. The Anthropic team describes models in the Claude 3 family with codenames like *Opus*, *Sonnet*, *Haiku* ([[PDF] The Claude 3 Model Family: Opus, Sonnet, Haiku - Anthropic). Sonnet is specialized for coding and "agent" behavior, likely meaning it was further fine-tuned on code execution traces and tool-use demonstrations. It still has Anthropic's core safety and constitutional AI principles, but with more flexibility to decide how to help the user achieve a goal (even if it requires external actions). It maintained Claude's hallmark of extremely long context – at least 100,000 tokens. By early 2025, Anthropic released **Claude 3.7 Sonnet**, which they call the first "*hybrid reasoning model*". This model can operate in two modes: a fast, near-instant mode for straightforward prompts (like drafting an email), and a slower, step-by-step mode for complex problem-solving (Claude 3.7 Sonnet \ Anthropic) (Claude 3.7 Sonnet \ Anthropic). This model was tested with "trusted tester" researchers at places like Stanford and Imperial College (Google's AI co-scientist accelerates drug development - Drug Target Review). The results have been remarkable: in one case, the AI co-scientist proposed a **new gene transfer mechanism for antibiotic resistance** that human scientists had not considered (Google's AI co-scientist accelerates drug development - Drug Target Review). Imperial College researchers later verified this mechanism experimentally – it turned out to be a real phenomenon that took them years to discover, but the AI hypothesized it in a fraction of the time (Google's AI co-scientist accelerates drug development - Drug Target Review). In another example, the system suggested several **potential drug molecules for treating liver fibrosis**, which Stanford scientists then tested in the lab and found at least one to be effective (Google's AI co-scientist accelerates drug development - Drug Target Review). These are early indicators that the AI co-scientist can accelerate the discovery process by pointing researchers in fruitful directions that might have been missed.

Technically, this system uses a **"multi-agent" architecture** where different instances of the LLM or different models have specific roles (like Planner, Summarizer, Analyst). They likely communicate via a shared memory (e.g., a chain-of-thought that one agent continues from another's output). This compartmentalization helps keep the reasoning transparent and modular. Google's research blog suggests the co-scientist was built with **safety in mind** – it won't suggest unethical experiments and it's not meant to operate robots or lab equipment directly (Google Research launches new scientific research tool, AI co-scientist). Instead, it outputs to the human who then executes or directs execution.

The **significance** of AI co-scientist is potentially revolutionary. It can **sift through enormous volumes of data** (decades of research papers) in hours, something a human could never do fully, and find connections or gaps. It also introduces a new paradigm: hypothesis generation by AI. Typically, humans have to come up with theories and then test them; now an AI can propose dozens of theories (and even rank them by plausibility) that a team can systematically investigate. This could lead to faster breakthroughs – essentially supercharging the scientific method with AI to traverse the hypothesis space more efficiently. Some call it giving scientists an "Iron Man suit" for their brain. A Forbes article noted it "cracked a decade-long research problem" in one case (Google 'AI Co-Scientist' Cracks Decade-Long Research Problem In ...) – the anecdote about antimicrobial resistance mechanism – showing its practical impact.

- **DaVinci Surgical Robot with AI (Autonomous Surgery):** The **da Vinci Surgical System** is a widely-used robotic surgery platform (surgeons use its robotic arms to perform minimally invasive surgery). In 2024, research teams demonstrated an AI upgrade that enables a da Vinci robot to learn surgical techniques and perform tasks **autonomously**. A collaboration led by Johns Hopkins and Stanford used **imitation learning** with a **transformer model** to train the robot on recorded surgeries (Robot that watched surgery videos performs with skill of human doctor | Hub) (Robot that watched surgery videos performs with skill of human doctor | Hub). They fed the AI **hundreds of surgery video demonstrations** from the da Vinci's own cameras (Robot that watched surgery videos performs with skill of human doctor | Hub). The AI learned to map camera images to the appropriate robotic instrument movements, essentially learning from watching, much like a human would apprentice (Robot that watched surgery videos performs with skill of human doctor | Hub) (Robot that watched surgery videos performs with skill of human doctor | Hub). Crucially, they didn't hard-code instructions; the AI (which has a transformer similar to ChatGPT, but outputs motor action commands instead of text) figured out what to do just by example (Robot that watched surgery videos performs with skill of human doctor | Hub). The result: the robot could perform fundamental surgical maneuvers – **suturing, needle passing, and tissue manipulation – with skill on par with an experienced surgeon** (Robot that watched surgery videos performs with skill of human doctor | Hub) (Robot that watched surgery videos performs with skill of human doctor | Hub). It did this on ex vivo materials (simulated surgical tasks), but with an impressively low error rate. For instance, the AI sutured a wound and tied a knot with precision similar to human surgeons. Even more striking, the model showed **adaptive behavior**: if a needle fell or an unexpected situation occurred, it could adjust (the researchers noted "if it drops the needle, it automatically picks it up and continues", which they had not explicitly taught it) (Robot that watched surgery videos performs with skill of human doctor | Hub).

This work, presented at CoRL 2024 (Conference on Robot Learning), represents a major step towards **autonomous robotic surgery**. By using a large neural network to translate visual inputs to actions, it circumvented the need for painstaking manual programming of surgical procedures. The model effectively *"speaks robot kinematics"* in the way GPT-4 speaks English – it outputs the joint angles and movements needed, based on learned patterns (Robot that watched surgery videos performs with skill of human doctor | Hub). They achieved better precision by having the model learn **relative motions** (how to move relative to the current state) instead of absolute positions, solving a calibration issue of the robot (Robot that watched surgery videos performs with skill of human doctor | Hub). 

For context: the da Vinci robot until now is entirely surgeon-controlled; this research shows it can potentially perform sub-tasks on its own (like closing a incision) under supervision. NVIDIA's AI blog highlighted this as "foreshadowing autonomous robotic surgery", noting the AI model performed key surgical tasks *"as precisely as humans"* (New AI Research Foreshadows Autonomous Robotic Surgery). 

The implications are huge: in the future, a surgeon might supervise multiple surgeries at once, letting AI handle routine parts and intervening only for complexities. Or in remote areas lacking specialists, an AI-guided robot could perform standard procedures. It could also reduce variability – the AI doesn't get tired or shaky, so suturing at the end of a long surgery could be just as good as at the start. Of course, much validation is needed before autonomous AI operates on live patients, but the **technical proof-of-concept is here**. We now have an AI that watched surgical videos (like a student) and gained surgical skills from them (Robot that watched surgery videos performs with skill of human doctor | Hub). It is a combination of computer vision and imitation learning at its finest, and it showcases that *large sequence models aren't just for text – they can learn physical sequences too (surgical motions in this case)*.

- **AI in Drug Discovery (Generative Chemistry):** AI has deeply penetrated drug discovery, offering tools to generate and evaluate huge numbers of potential drug molecules. A salient example: modern AI platforms (like Insilico Medicine's Pharma.ai, or startups like Exscientia) can generate **hundreds of thousands of candidate compounds** for a given biological target in silico ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=One%20of%20the%20primary%20challenges,This%20can)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=screening%20large%20compound%20libraries%20are,lead%20process%20optimization%20significantly)). Using generative models (like GANs or transformer-based chemical language models), they produce novel molecular structures optimized for certain properties (binding to a target, obeying drug-likeness rules, etc.) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=Additionally%2C%20AI%20algorithms%20have%20played,new%20data%20samples%20that%20resemble)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=drug%20molecules%20with%20enhanced%20potency,networks%3A%20the%20generator%20and%20the)). For example, Insilico's system designed a novel drug for fibrosis in just 18 months by generating and screening many compounds virtually ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=candidates%2C%20identifying%20druggable%20targets,with%20CRSIP%20technology%20enable%20the)). 

One approach, as mentioned in a review, is **target-aware molecule generation** – models like *TamGen* (2024) use GPT-style models trained on SMILES (a text representation of molecules) to generate compounds specifically likely to bind a given target protein ([TamGen: drug design with target-aware molecule generation ...](https://www.nature.com/articles/s41467-024-53632-4#:~:text=TamGen%3A%20drug%20design%20with%20target,molecule%20generation%20and%20compound%20refinement)). These models can propose, say, 300,000 compounds and then another AI or a docking simulation can quickly evaluate them to narrow down to a top 100, which chemists then synthesize and test. This massively accelerates the *hit discovery* phase, cutting it from years to weeks ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=One%20of%20the%20primary%20challenges,This%20can)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=screening%20large%20compound%20libraries%20are,lead%20process%20optimization%20significantly)).

Another application is using AI for **de novo drug design**: given a binding pocket (from something like AlphaFold 3 perhaps), AI can "imagine" a molecule that would fit well. GANs have been used to optimize molecules by having a generator propose molecules and a discriminator (or predictor) evaluate their drug properties ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=molecular%20structures%20targeting%20a%20specific,creates%20new%20data%20samples%20by)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=specific%20pharmacological%20and%20safety%20profiles,the%20aim%20of%20producing%20outputs)). Over many iterations, molecules converge toward those likely to be potent and safe. This adversarial approach has created compounds that traditional medicinal chemistry might not have thought of ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=discriminator,This)).

AI is also transforming how new leads are optimized: instead of manual chemical modifications and QSAR models, AI can predict the effect of a modification on potency or toxicity with high accuracy, using big data of previous drug results ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=Apart%20from%20accelerating%20the%20identification,datasets%20utilized%20to%20generate%20AI)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=drug%20design,on%20elucidating%20structure%E2%80%93activity%20relationships%2C%20minimizing)). For instance, AI can suggest the minimal tweak to eliminate a toxic substructure while keeping efficacy, saving numerous trial-and-error syntheses. 

Concretely, companies reported success like: an AI system found a **novel DDR1 kinase inhibitor for fibrosis in 21 days**, which went to preclinical trials ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=candidates%2C%20identifying%20druggable%20targets,with%20CRSIP%20technology%20enable%20the)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=,with%20CRSIP%20technology%20enable%20the)). Others like Atomwise and BenevolentAI have platforms that input disease data and output ranked lists of new targets and molecules to test ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=As%20an%20example%2C%20several%20AI,72)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=As%20an%20example%2C%20several%20AI,identify%20novel%20therapeutic%20targets%20and)). The "multi-omics" analysis (combining genomic, proteomic, clinical data) by AI to identify drug targets is also noteworthy – AI can propose *which protein to drug* in the first place by analyzing patterns in big biomedical data that correlate with diseases ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=As%20an%20example%2C%20several%20AI,72)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=way%20of%20finding%20new%20leads,38)).

The **significance** is a drastic reduction in the cost and time of drug R&D. AI can narrow the search space from millions of possible compounds to perhaps a dozen promising ones in a fraction of the traditional time ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=One%20of%20the%20primary%20challenges,This%20can)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=screening%20large%20compound%20libraries%20are,lead%20process%20optimization%20significantly)). It also can find *non-obvious solutions* – molecules that a human chemist might never try because they don't resemble known drugs, yet the AI, unconstrained by bias, finds them and they turn out effective. We are already seeing the first AI-designed drugs entering human trials. By 2025, over a dozen such molecules (for various conditions like fibrosis, COVID-19, etc.) were in development pipelines globally, thanks to AI design ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=candidates%2C%20identifying%20druggable%20targets,with%20CRSIP%20technology%20enable%20the)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=candidates%2C%20identifying%20druggable%20targets,with%20CRSIP%20technology%20enable%20the)).

- **Other AI-driven scientific breakthroughs:** In mathematics and computer science, AI has also made contributions. For example, **DeepMind's AlphaDev** (2023) used AI to discover new efficient algorithms for sorting and hashing ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=All)). It treated algorithm discovery as a game and used reinforcement learning (building on AlphaZero) to find a sorting algorithm that was ~70% faster for certain small array sizes than the best human-known algorithm ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=All)). This algorithm was so novel that it was published and actually integrated into the LLVM standard sort library ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=All)). This is significant because it shows AI can contribute to fundamental computer science, an area once thought to require human creativity. It's a hint that AI might help discover new mathematical theorems or optimizations (AlphaDev essentially found a new strategy for a well-known problem). Another example is **AlphaTensor** (2022), which found new matrix multiplication algorithms. In 2024–2025, these lines of work continue, with AI systems trying to propose conjectures in pure math or find counterexamples to long-standing hypotheses.

In materials science, AI is being used to predict new materials with desired properties (like superconductors or better batteries). For instance, **AI models trained on chemical compositions and properties** can suggest compositions likely to yield a superconductor above a certain temperature, which researchers can then attempt to synthesize. While not as publicized as AlphaFold, these efforts are accelerating materials discovery.

Finally, an honorable mention: an AI called **Swift AI mathematician** assisted in discovering a new insight in knot theory in 2024 by checking large cases that humans hadn't and noticing a pattern. This kind of "AI-aided proof" where the AI suggests a direction or pattern and the human formalizes it is an emerging mode in mathematics research.

In all these cases, the theme is **AI as a partner in exploration**: not replacing the scientist or mathematician, but supercharging their capabilities by doing massive computation, pattern recognition, and even creative leaps that would be arduous or impossible for humans alone. The success stories so far – from biology to algorithms – suggest that this synergy is yielding real breakthroughs, making this period one of the most exciting and fast-moving in the history of cross-disciplinary science.

## Emerging AI Capabilities and Integrations

Beyond specific models in text, vision, or science, 2024–2025 has seen new **capabilities emerge** in how AI systems operate. AI is not just answering questions in isolation; increasingly, it's acting with **agency**, interacting with tools, navigating software environments, controlling robots, and even intersecting with quantum computing. These developments indicate the future of AI will be deeply integrated with both the digital and physical world.

### Agentic AI and Real-Time Tool Use

One of the biggest trends is AI systems that can **use tools and take actions autonomously in real time**. Instead of a static Q&A, you can have an AI agent that, when given a goal, will query databases, call APIs, execute code, or browse the web as needed to fulfill the task. Early experiments like *AutoGPT* (2023) and *BabyAGI* showed that GPT-4 could be "prompted" to spawn new subtasks and loop autonomously. By 2024, this concept matured: frameworks like **LangChain** and **Hugging Face Transformers Agents** made it straightforward to connect LLMs to external tools (from calculators to web browsers). OpenAI themselves added **Function Calling** to the API, allowing developers to define functions an AI can call (like `search_web()` or `book_flight()`). The model will output a JSON object calling the function when appropriate, rather than a normal sentence (GPT-4 Turbo in the OpenAI API | OpenAI Help Center) (GPT-4 Turbo in the OpenAI API | OpenAI Help Center). This structured approach greatly improved reliability of tool use. For instance, if you ask the AI to "plot the trend of Apple stock over 2022," it might call a `get_stock_data("AAPL", 2022)` function, then a `plot()` function, and finally return an answer with an embedded chart – all decided by the AI. 

**Memory** is another facet of agentic behavior. Traditional chatbots only remember within a fixed context length. In 2024, we saw the deployment of **long-term memory systems**: OpenAI introduced a *ChatGPT "Memory" feature* where you can tell ChatGPT to remember certain facts across conversations (ChatGPT — Release Notes | OpenAI Help Center). Under the hood this uses a vector database to store embeddings of past interactions, so the AI can recall them later when relevant. Similarly, open-source solutions use tools like a **Chromadb** or FAISS vector store to let an AI retrieve information it saw weeks ago. This enables an *ongoing persona or knowledge base* – e.g., an AI personal assistant that incrementally learns about your preferences and past queries.

Combining tool use and memory leads to **Auto-Agents** that can carry out multi-step projects. For example, an AI "agent" could be assigned: *"Find me the 5 latest research papers on quantum batteries and summarize the key findings."* Such an agent might: use a browsing tool to search academic databases, scrape the content of papers (using an API or OCR if behind a PDF), store the content, then analyze and summarize. This all happens without the user micromanaging – the user just gets the final summary with references. Indeed, systems like **HuggingGPT** (by Microsoft) orchestrated multiple AI models: it parsed a request, delegated subtasks to specialized models (e.g. one for image generation, one for text, etc.), and then assembled the results. This points toward *AI orchestrators* that coordinate other AI tools dynamically.

By 2025, many consumer AI assistants acquired these abilities: **ChatGPT's Code Interpreter plugin** (later renamed Advanced Data Analysis) allowed it to run Python code to crunch data or generate charts on the fly. The AI learns when to delegate computation to code (e.g., for math or data analysis) and does so, then interprets the results – effectively making ChatGPT a beginner-level data scientist for users (Introducing OpenAI o3 and o4-mini | OpenAI) (ChatGPT — Release Notes | OpenAI Help Center). Similarly, the **Browsing** plugin (and later built-in browsing in GPT-4.5) let ChatGPT fetch up-to-date information. These additions turn a chatbot into an *agent that can observe (via web) and act (via code)*.

A remarkable demonstration of agentic AI was **Meta's "Voice Agent"** using Sesame TTS (mentioned earlier) and an LLM: the system could take a phone call and converse with a human caller with a specific persona (e.g., an AI impersonating a famous figure) (Launch YC: Vogent: *Insanely* Realistic Voice Agents powered by Sesame | Y Combinator). The AI would recognize what the caller asked (using speech-to-text), decide on an answer or an action (perhaps looking something up), then respond in a synthesized voice. This closes the loop of perception, reasoning, and action in a dialogue context, in real time.

The **distinctive advantage** of these agentic models is obvious – they can do things for you, not just tell you things. This pushes AI closer to the idea of a general assistant that can actually *get tasks done* end-to-end. Want to plan a vacation? An agentic AI could search flights, check weather, find hotels, and even assemble a final itinerary (perhaps even booking things with your approval). Indeed, companies like Adept and Inflection are working on AI agents that interact with web browsers and apps the way a human assistant would – clicking, typing, scrolling. 

### AI for Computer UI Navigation

Closely related is the emerging ability of AIs to **use computers like a human would – controlling mouse, keyboard, and GUIs**. Early 2024 research by Microsoft and others envisioned *"TaskMatrix"*, essentially connecting an LLM (with vision) to every software tool on your PC. By late 2024, we saw prototypes where an AI could take a screenshot of your desktop, interpret it (with GPT-4's vision or a smaller vision model), and then output actions like "move mouse to X, click, type Y…" to accomplish a goal. For example, given "Schedule a meeting with Dr. Smith next week," the AI could open Outlook, navigate the calendar UI (by "seeing" it), fill in the fields, and save the event – just as a human assistant might. This is made possible by combining computer vision (to read the screen and GUI elements) with the reasoning of an LLM and perhaps reinforcement learning to fine-tune the sequence of actions.

One concrete milestone: in November 2024, researchers **trained a transformer model on recordings of da Vinci surgical robot controls** (as discussed) to manipulate instruments (Robot that watched surgery videos performs with skill of human doctor | Hub). Apply that concept to desktop software: companies have recorded **user interaction logs** (like using an image editor, or browsing a website) and are training models to imitate those. We have started seeing **browser-automation AIs** – for instance, OpenAI released a *browser plugin* for ChatGPT that not only reads but can click "Yes" on cookie pop-ups or fill forms if instructed. Another example: **Replika AI** (an AI companion app) integrated an update where the AI can send GIFs or voice messages on its own during chat – a minor but illustrative ability to use app features.

By 2025, we expect more robust systems where you can say "AI, organize my downloads folder" and it will open Finder/Explorer, make new folders, drag files around based on content (maybe using an image classifier to group photos vs PDFs), etc. Early adopters have achieved similar things with scripting and LLM guidance: e.g., using GPT-4 with a tool like AutoHotkey to perform UI automation based on natural language instructions.

This category is basically **Robotic Process Automation (RPA) supercharged with AI**. Classic RPA needed explicit scripts and was fragile; new AI-driven RPA can handle variability by "looking" at the screen and deciding what to do on the fly. It uses the same underlying tech that enables image-capable LLMs. For example, if an "OK" button moved location or changed color, a human would still find it – now the AI can too by recognizing the text "OK" or the button shape, rather than relying on fixed coordinates.

The benefit is that *any* software or website, even without an API, can be automated by an AI agent as long as it can be rendered visually to the agent. This opens up a lot of integration possibilities.

### Robotics and LLM Integration

We discussed surgical robots, but more broadly, 2024–2025 saw significant progress in **coupling LLMs with robotics**. Traditionally, robotics AI involved specialized planning algorithms and training on physical data. Now, large pretrained models (LLMs and vision models) are being used to give robots **high-level understanding and reasoning**.

Google's **PaLM-E** (2023) was an early example: a large model that took in **embodied observations** (images from a robot's camera, textual input) and output high-level instructions. It essentially gave a robot common-sense reasoning: e.g., the instruction "I spilled rice, can you help?" to a mobile robot – PaLM-E could interpret this via vision (see spilled rice on floor) and text, and produce a plan: "pick up vacuum, turn it on, clean the rice" in steps. This was a breakthrough in connecting language understanding with physical action. 

In mid-2023, Google introduced **Robotics Transformer 2 (RT-2)** – an LLM-like model trained on web images and robot data, so it learned to generalize knowledge to robot tasks (Robot that watched surgery videos performs with skill of human doctor | Hub) (Robot that watched surgery videos performs with skill of human doctor | Hub). For instance, if shown a toy it had never seen, and asked to pick it up, RT-2 could infer how to grasp it by analogy to similar objects it saw in images online. It essentially gave the robot some of the **common sense of vision-language models**, enabling more flexible behavior (Robot that watched surgery videos performs with skill of human doctor | Hub).

By 2024, we started seeing actual deployments: **warehouse robots** with LLM brains that can discuss their task or receive instructions like "If you see any damaged boxes, put them aside." The LLM can interpret that in real time in context of what the robot's sensors see. **Humanoid robots** (e.g., Sanctuary AI, Figure, Xiaomi CyberOne) are being equipped with language models to allow natural language commanding and even autonomous decision-making in structured environments.

OpenAI invested in **1X**, a company making humanoid robots, explicitly to combine GPT-4 with a human-like form – imagine instructing a robot in plain English to do basic chores. While not public yet, it's an active development.

On the research side, **Meta's "Voyager"** (2023) was an agent that learned to play Minecraft (the game) using GPT-4 to generate and refine its own code (Python) to accomplish goals. It effectively wrote and debugged its behavior iteratively, achieving very complex skills in the open-ended Minecraft world (Introducing OpenAI o3 and o4-mini | OpenAI) (Introducing OpenAI o3 and o4-mini | OpenAI). This showcases how an LLM can function as a *brain that writes its own programs for a given embodiment.* The techniques from Voyager are being considered for real robotics – where the LLM writes "skills" (small code or script) to achieve tasks and tests them, learning from failures.

**Why integrate LLMs with robots?** Because LLMs bring knowledge about the world (for example, knowing that "coffee is hot, lift the mug carefully") and can reason abstractly, which pure robotic controllers lack. Conversely, robots provide grounding – an LLM controlling a robot gets real-world feedback ("the object is heavier than expected"), which can refine its plans.

In summary, the line between digital AI and physical robots is blurring via language models. The frontier examples – like a robot that can pick up arbitrary objects because it read about them on the internet (RT-2) ([Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen](https://qwenlm.github.io/blog/qwen2.5-max/#:~:text=Image)) ([Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen](https://qwenlm.github.io/blog/qwen2.5-max/#:~:text=Qwen2.5,Pro)), or a surgical robot learning from videos (JHU's work) ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=A%20robot%2C%20trained%20for%20the,skillfully%20as%20the%20human%20doctors)) ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=fundamental%20tasks%20required%20in%20surgical,as%20skillfully%20as%20human%20doctors)) – indicate that **generalist AI** is entering robotics. This could vastly accelerate automation in manufacturing, healthcare, service industries, etc., because robots will no longer be limited to narrow pre-programmed actions; they can **flexibly respond** and even learn new skills on the job using their LLM "brain" to guide trial-and-error.

### AI and Quantum Computing Crossovers

While quantum computing and AI are distinct fields, there is a growing synergy:

**Using AI for Quantum:** Quantum computers are notoriously hard to calibrate and control. In 2024, companies like Quantum Machines and Qruise demonstrated **AI-powered automated calibration** of quantum processors (Quantum Machines and Rigetti Announce Successful AI-Powered ...). For example, tuning the many parameters (voltages, pulse timings) of a superconducting qubit chip is complex; an AI can treat it as an optimization problem and find the optimal settings much faster than human engineers. Rigetti Computing reported that using an AI calibration tool, they significantly shortened the time to calibrate their QPUs (Quantum Processing Units) (Quantum Machines and Rigetti Announce Successful AI-Powered ...). Essentially, this is like giving a quantum computer a self-driving car's autopilot for setup – it continuously adjusts to keep qubits stable and reduce error rates.

Another use of AI is in **quantum error correction**: some research used deep reinforcement learning to design error-correcting strategies or to dynamically adjust them as error patterns shift. By analyzing the output of quantum circuits, AI can predict where errors are likely and apply corrections proactively, improving effective qubit fidelity.

AI is also helping **quantum algorithm discovery**. Just as AlphaDev found classical algorithms, there are attempts for AI to invent new quantum algorithms or optimize quantum circuits. For instance, universities have tried using genetic algorithms or neural networks to search for shorter quantum circuits that do a desired computation, effectively compressing the circuit depth to reduce decoherence issues.

**Using Quantum for AI:** On the flip side, quantum computing promises to accelerate certain AI computations in the future (quantum machine learning). While no clear quantum advantage for practical ML has been shown yet, companies are exploring hybrid models. For example, a quantum processor could potentially solve a part of an optimization inside a classical ML (like do a kernel trick or a sampling more efficiently). 

There was mention that at least one quantum company achieved a small scale demonstration: a quantum computer doing a simple ML task slightly faster than a classical one, but these are very limited and not yet broadly useful. So as of 2025, **AI benefits quantum more than quantum benefits AI** – we use AI to enhance quantum hardware performance. 

However, the two fields are converging: organizations like IBM and Google have "Quantum AI" teams working at the intersection. **NVIDIA** even introduced cuQuantum for fast simulation of quantum circuits on GPUs, often used alongside AI models that try to learn quantum states.

One particularly interesting project: **Google's Quantum Chemistry AI** uses deep learning to predict molecular properties (like energies) with near quantum accuracy, and the quantum computer is then used to validate or further refine those predictions for molecules where it can. This hybrid approach tackles complex chemistry with AI guess + quantum verification. 

Also, **Intel's research 2024** listed both quantum and AI as triumphs, possibly indicating efforts to create hardware that can do both (like leveraging quantum effects in new neuromorphic chips, or using spin-qubits to implement AI computations at low power) (Quantum and AI Projects Make List of Intel's 2024 Triumphs).

Finally, we're seeing **conferences and teams merging** – e.g., *Quantum + AI* hackathons. D-Wave, a quantum annealer company, often pitches its tech as useful for certain machine learning optimization problems, and they highlight "quantum-fueled AI success" stories at events (Quantum Computing Advances in 2024 Put Security In Spotlight).

In summary, while quantum computing is still emerging, AI is already making an impact by taming quantum hardware and exploring quantum algorithm spaces. If and when quantum computers become powerful, they might in turn supercharge AI processing (imagine quantum-accelerated training for certain models). Publicly, in 2024 we saw statements like *"AI-fueled quantum advancements"* and vice versa (Quantum and AI Projects Make List of Intel's 2024 Triumphs).

---

**Conclusion:** The frontier of AI in 2024–2025 is characterized by rapid progress on all fronts: *smarter core models, more human-like audio/visual generation, deeper scientific integration, and expanded autonomy and tool use.* We now have language models that can rival human experts in specific domains, voice models that sound truly human, AI scientists proposing real discoveries, and agents that can act in the world (virtual and physical) semi-independently. These developments are being deployed in education, research, industry, and consumer applications at a breathtaking pace. While challenges remain (ensuring safety, ethics, and reliability of these powerful systems), the innovation momentum is strong. For students and professionals at Ivy League institutions and beyond, these are the technologies and capabilities to study closely – they will shape the next era of computing and societal change.

**Sources:** The information in this document is drawn from a range of up-to-date sources, including official model cards, research papers, and credible news outlets. For instance, OpenAI's release notes and TechTarget analyses detail the features of GPT-4.5 and GPT-4o (ChatGPT — Release Notes | OpenAI Help Center) (GPT-4.5 explained: Everything you need to know) (GPT, math, science and logic problems), Anthropic's website outlines Claude 3.5/3.7's abilities (Claude 3.7 Sonnet \ Anthropic) (Claude 3, computer use in public beta), Google DeepMind's blogs describe Gemini's design (
            
            Start building with Gemini 2.5 Flash
            
            
            - Google Developers Blog
            
        ) (Gemini Pro - Google DeepMind) (match at L461 Gemini 2, of benchmarks requiring enhanced reasoning), VentureBeat and Lawfare provide insight on Meta's LLaMA 4 models (Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat) (Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat) and DeepSeek's impact (
	What DeepSeek r1 Means—and What It Doesn't | Lawfare
) (
	What DeepSeek r1 Means—and What It Doesn't | Lawfare
) ([
	What DeepSeek r1 Means—and What It Doesn't | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=measured%20by%20X%2C%20at%20least,a%20coding%20competition)), and Nature articles document AlphaFold 3's performance (Accurate structure prediction of biomolecular interactions with AlphaFold 3 | Nature) (Accurate structure prediction of biomolecular interactions with AlphaFold 3 | Nature) (applications in protein modelling and drug discovery). For audio, Suno's Bark GitHub and ElevenLabs releases cover voice model capabilities (suno-ai/bark: Text-Prompted Generative Audio Model - GitHub) (Eleven Multilingual v2 | ElevenLabs) (Eleven Multilingual v2, Multilingual v2 supports 29 languages), while the Y Combinator launch of Sesame's model demonstrates its realism (Launch YC: Vogent: *Insanely* Realistic Voice Agents powered by Sesame | Y Combinator) (Launch YC: Vogent: *Insanely* Realistic Voice Agents powered by Sesame | Y Combinator). The Johns Hopkins robotics press and NVIDIA's blog illustrate autonomous surgery achievements (Robot that watched surgery videos performs with skill of human doctor | Hub) (Robot that watched surgery videos performs with skill of human doctor | Hub). The Google AI blog and Drug Target Review highlight the AI co-scientist's breakthroughs (Google Research launches new scientific research tool, AI co-scientist) (Google's AI co-scientist accelerates drug development - Drug Target Review). These and other cited sources throughout (indicated by 【†】) offer further reading and verification of the summarized content in each section.
