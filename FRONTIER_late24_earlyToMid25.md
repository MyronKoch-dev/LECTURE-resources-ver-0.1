Perfect — I’ll pull together the most authoritative and bleeding-edge updates across core AI models, tool-augmented agents, voice/audio generation, scientific discoveries, and emerging tech (like robotics and quantum). I’ll format this as a well-structured document, with concise descriptions *plus* deeper dives and links where possible. Tables will be used for comparison where clarity helps. You’ll be able to teach from this, onboard both technical and non-technical students, and inspire the sharpest minds.

I’ll kick this off now and let you know as soon as the draft is ready for review.

# Frontier AI and Adjacent Technologies (2024–2025): A Comprehensive Overview

## Core Language Models (LLMs) of 2024–2025

The years 2024 and 2025 have seen a wave of advanced large language models, with major releases focusing on improved reasoning, larger context windows, multimodality, and specialized capabilities. Below is a comparison of key LLMs released in this period:

| **Model**               | **Developer**    | **Release**         | **Modalities**              | **Tool Use / Memory**               | **Distinct Strengths**                                    |
|-------------------------|-----------------|---------------------|-----------------------------|-------------------------------------|-----------------------------------------------------------|
| **GPT-4o (Omni)**       | OpenAI          | May 13, 2024 ([ChatGPT — Release Notes | OpenAI Help Center](https://help.openai.com/en/articles/6825453-chatgpt-release-notes#:~:text=May%2013%2C%202024)) | Text, Vision (images); Voice-ready | Yes – function calling, plugins; long-term memory support ([ChatGPT — Release Notes | OpenAI Help Center](https://help.openai.com/en/articles/6825453-chatgpt-release-notes#:~:text=Apr%2029%2C%202024)) | GPT-4-level intelligence at higher speed ([ChatGPT — Release Notes | OpenAI Help Center](https://help.openai.com/en/articles/6825453-chatgpt-release-notes#:~:text=GPT,more%20about%20the%20announcement%20here)); enhanced coding & instruction-following ([ChatGPT — Release Notes | OpenAI Help Center](https://help.openai.com/en/articles/6825453-chatgpt-release-notes#:~:text=GPT,that%20successfully%20compile%20and%20run)); 128K context ([GPT-4 Turbo in the OpenAI API | OpenAI Help Center](https://help.openai.com/en/articles/8555510-gpt-4-turbo-in-the-openai-api#:~:text=GPT,for%20this%20model%20is%204096)). |
| **GPT-4o Mini**         | OpenAI          | July 18, 2024 ([ChatGPT — Release Notes | OpenAI Help Center](https://help.openai.com/en/articles/6825453-chatgpt-release-notes#:~:text=July%2018%2C%202024)) | Text, Vision                 | Yes – function calling ([ChatGPT — Release Notes | OpenAI Help Center](https://help.openai.com/en/articles/6825453-chatgpt-release-notes#:~:text=We%E2%80%99re%20introducing%20GPT,3.5%20Turbo)); memory | Most capable “small” model – outperforms GPT-3.5 Turbo ([ChatGPT — Release Notes | OpenAI Help Center](https://help.openai.com/en/articles/6825453-chatgpt-release-notes#:~:text=We%E2%80%99re%20introducing%20GPT,3.5%20Turbo)); cost-efficient. |
| **ChatGPT-4.5**         | OpenAI          | Feb 2025 ([GPT-4.5 explained: Everything you need to know](https://www.techtarget.com/whatis/feature/GPT-45-explained-Everything-you-need-to-know#:~:text=At%20the%20time%20of%20its,based%20LLM)) ([GPT-4.5 explained: Everything you need to know](https://www.techtarget.com/whatis/feature/GPT-45-explained-Everything-you-need-to-know#:~:text=GPT,will%20roll%20out%20broader%20availability)) | Text (images/files accepted ([Introducing GPT-4.5 | OpenAI](https://openai.com/index/introducing-gpt-4-5/#:~:text=GPT%E2%80%914,AI%20%E2%80%9Cjust%20works%E2%80%9D%20for%20you))) | Yes – built-in web browsing ([Introducing GPT-4.5 | OpenAI](https://openai.com/index/introducing-gpt-4-5/#:~:text=Starting%20today%2C%20ChatGPT%20Pro%20users,Edu%20users%20the%20following%20week)), code canvas, tools; 32K+ context | Largest OpenAI model (GPT-4.5) with improved factual accuracy ([Introducing GPT-4.5 | OpenAI](https://openai.com/index/introducing-gpt-4-5/#:~:text=often%20drew%20on%20classical%20themes,stories%20from%20antiquity%20for%20inspiration)) and emotional intelligence ([GPT-4.5 explained: Everything you need to know](https://www.techtarget.com/whatis/feature/GPT-45-explained-Everything-you-need-to-know#:~:text=the%20GPT%20series%20models,more%20detail%20than%20prior%20models)) ([GPT-4.5 explained: Everything you need to know](https://www.techtarget.com/whatis/feature/GPT-45-explained-Everything-you-need-to-know#:~:text=%2A%20Emotional%20intelligence.%20GPT,broader%20knowledge%20base%20for%20answering)); more natural, conversational responses ([GPT-4.5 explained: Everything you need to know](https://www.techtarget.com/whatis/feature/GPT-45-explained-Everything-you-need-to-know#:~:text=While%20previous%20iterations%2C%20such%20as,its%20prior%20GPT%20class%20models)) ([GPT-4.5 explained: Everything you need to know](https://www.techtarget.com/whatis/feature/GPT-45-explained-Everything-you-need-to-know#:~:text=,summarize%20large%20documents%20and%20volumes)). |
| **Claude 3.5 “Sonnet”** | Anthropic       | Oct 22, 2024 ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Read%20more%20,5%20Sonnet)) | Text, **Vision, Code**, UI control | Yes – API (Claude-Next) with tool use and “computer use” features ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Claude%203,computer%20use%20in%20public%20beta)) ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Read%20more%20,5%20Sonnet)) | “Agentic” model: state-of-the-art coding assistance and ability to operate computer tools ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Claude%203,computer%20use%20in%20public%20beta)); 100K–200K context window; hybrid reasoning (chain-of-thought + reflexive) in Claude 3.x ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Claude%203,computer%20use%20in%20public%20beta)) ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Hybrid%20reasoning%20model%2C%20state,use%2C%20and%20200K%20context%20window)). |
| **Claude 3.7 “Sonnet”** | Anthropic       | Feb 24, 2025 ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Announcements)) | Text, Vision, UI, 200K context ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Hybrid%20reasoning%20model%2C%20state,use%2C%20and%20200K%20context%20window)) | Yes – fine-grained control of reasoning steps; tool use via API | First *hybrid reasoning* model, mixing fast “instinct” with step-by-step “thinking” ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Feb%2024%2C%202025)); top coding performance; near real-time responses with user-controlled “thinking time” ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Hybrid%20reasoning%20model%2C%20state,use%2C%20and%20200K%20context%20window)) ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Claude%203,a%20variety%20of%20use%20cases)). |
| **Gemini 2.0 “Flash”**  | Google DeepMind | Mid-2024 (Preview)   | Text (code), **some Vision**       | Yes – limited; fast API responses | Fast, cost-efficient model emphasizing speed/$$ tradeoff. Early Gemini with moderate reasoning. |
| **Gemini 2.5 Flash**    | Google DeepMind | Apr 17, 2025 ([
            
            Start building with Gemini 2.5 Flash
            
            
            - Google Developers Blog
            
        ](https://developers.googleblog.com/en/start-building-with-gemini-25-flash/#:~:text=Start%20building%20with%20Gemini%202,Flash)) | Text, Vision* (API)         | Yes – **“fully hybrid” reasoning** (toggle thinking) ([
            
            Start building with Gemini 2.5 Flash
            
            
            - Google Developers Blog
            
        ](https://developers.googleblog.com/en/start-building-with-gemini-25-flash/#:~:text=match%20at%20L150%20still%20prioritizing,with%20thinking%20off%2C%20developers%20can)); tools via API | **Hybrid reasoning** model: developers can turn thorough “thinking” on/off and set reasoning “budget” ([
            
            Start building with Gemini 2.5 Flash
            
            
            - Google Developers Blog
            
        ](https://developers.googleblog.com/en/start-building-with-gemini-25-flash/#:~:text=match%20at%20L150%20still%20prioritizing,with%20thinking%20off%2C%20developers%20can)). Balances quality vs. latency; second-best performance after Pro ([
            
            Start building with Gemini 2.5 Flash
            
            
            - Google Developers Blog
            
        ](https://developers.googleblog.com/en/start-building-with-gemini-25-flash/#:~:text=match%20at%20L162%20comprehensive%20answers,5%20Pro)). |
| **Gemini 2.5 Pro**      | Google DeepMind | Early 2025 (Exp.→Preview) ([Gemini Pro - Google DeepMind](https://deepmind.google/technologies/gemini/pro/#:~:text=Start%20testing%20Gemini%202)) | Text, Vision, **Code**       | Yes – full tool use planned (incl. browsing) ([Gemini Pro - Google DeepMind](https://deepmind.google/technologies/gemini/pro/#:~:text=Watch%20Gemini%202,simulation%20of%20a%20reflection%20nebula)) ([Gemini models | Gemini API | Google AI for Developers](https://ai.google.dev/gemini-api/docs/models#:~:text=Gemini%20models%20,as%20analyzing%20large%20datasets%2C)) | **State-of-the-art reasoning & coding** – surpasses prior models on complex math/Codeforces ([Introducing OpenAI o3 and o4-mini | OpenAI](https://openai.com/index/introducing-o3-and-o4-mini/#:~:text=OpenAI%20o3%20is%20our%20most,for%20complex%20queries%20requiring%20multi)). Uses chain-of-thought natively (“thinking built-in”) ([Gemini - Google DeepMind](https://deepmind.google/technologies/gemini/#:~:text=Gemini%20,enhanced%20performance%20and%20improved%20accuracy)). Large context (≥100K). |
| **LLaMA 4 “Scout”**     | Meta (open-source) | Jan/Feb 2025 ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=Now%20we%20know%20the%20fruits,code%20sharing%20community%20Hugging%20Face)) ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=new%20Llama%204%20series%20of,code%20sharing%20community%20Hugging%20Face)) | Text, Vision, **Video**      | Partial – can integrate with tools via API (open) | 109B-param MoE model ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=Now%20we%20know%20the%20fruits,code%20sharing%20community%20Hugging%20Face)) (128 experts) open-sourced. **10 million-token context** ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=One%20headline%20feature%20of%20these,audio%20was%20not%20mentioned)) (for long documents). Multimodal (understands text, images, video) ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=generally%20more%20mean%20a%20more,and%20complex%20all%20around%20model)). Extremely efficient inference via MoE ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=All)). |
| **LLaMA 4 “Maverick”**  | Meta (open-source) | Jan/Feb 2025 ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=Now%20we%20know%20the%20fruits,code%20sharing%20community%20Hugging%20Face)) ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=new%20Llama%204%20series%20of,code%20sharing%20community%20Hugging%20Face)) | Text, Vision, Video         | Partial – open API or local use with tools | 400B-param MoE model ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=Now%20we%20know%20the%20fruits,code%20sharing%20community%20Hugging%20Face)) with **1 million-token context** ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=One%20headline%20feature%20of%20these,audio%20was%20not%20mentioned)). Open-weight release for self-hosting ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=new%20Llama%204%20series%20of,code%20sharing%20community%20Hugging%20Face)). First open model to rival frontier proprietary models, excelling in multilingual and expert knowledge tasks. |
| **DeepSeek R1**         | DeepSeek (China) | Jan 20, 2025 ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=On%20Jan,a%20coding%20competition)) | Text (strong reasoning)     | Yes – open-source code; can use external tools via API | **Chain-of-thought reasoning specialist**. Matches OpenAI’s best reasoning model (o1) on grad-level science, math (GPQA, AIME) ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=measured%20by%20X%2C%20at%20least,a%20coding%20competition)). Open-source weights/paper ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=What%E2%80%99s%20more%2C%20DeepSeek%20released%20the,apps%20like%20Gemini%20and%20Claude)). **Cost-efficient** (trained for <$6M) ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=On%20Jan,a%20coding%20competition)) ([DeepSeek explained: Everything you need to know](https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know#:~:text=,with%20OpenAI%2C%20which%20is%20proprietary)) and runs 27× cheaper than OpenAI’s model ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=Alongside%20the%20main%20r1%20model%2C,cost%20of%20OpenAI%E2%80%99s%20competitor%2C%20o1)). Distillations available for local use ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=and%20far%20ahead%20of%20competitor,apps%20like%20Gemini%20and%20Claude)). |
| **Qwen-2.5-Max**        | Alibaba Cloud   | Jan 28, 2025 ([Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen](https://qwenlm.github.io/blog/qwen2.5-max/#:~:text=with%20the%20recent%20release%20of,Max%20on%20Qwen%20Chat)) ([Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen](https://qwenlm.github.io/blog/qwen2.5-max/#:~:text=Today%2C%20we%20are%20excited%20to,Max%20on%20Qwen%20Chat)) | Text, Vision (Qwen-VL2)     | Yes – fine-tuning and API via Alibaba Cloud ([Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen](https://qwenlm.github.io/blog/qwen2.5-max/#:~:text=Today%2C%20we%20are%20excited%20to,Max%20on%20Qwen%20Chat)) ([Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen](https://qwenlm.github.io/blog/qwen2.5-max/#:~:text=including%20DeepSeek%20V3%2C%20GPT,Sonnet)) | **Mixture-of-Experts architecture** (extremely large-scale). Trained on 20 trillion tokens ([Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen](https://qwenlm.github.io/blog/qwen2.5-max/#:~:text=extremely%20large%20models%2C%20whether%20they,Max%20on%20%205%20Qwen)). **Outperforms DeepSeek V3** on many benchmarks ([Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen](https://qwenlm.github.io/blog/qwen2.5-max/#:~:text=Image)) ([Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen](https://qwenlm.github.io/blog/qwen2.5-max/#:~:text=Qwen2.5,Pro)); top Chinese and competitive global performance ([Qwen - Wikipedia](https://en.wikipedia.org/wiki/Qwen#:~:text=Qwen%20,1)) ([Qwen - Wikipedia](https://en.wikipedia.org/wiki/Qwen#:~:text=language%20models%20%20developed%20by,1)). Open-source 72B and smaller variants ([Qwen - Wikipedia](https://en.wikipedia.org/wiki/Qwen#:~:text=by%20Meta%20AI.,7)); larger models via API. |

*(**Modalities legend**: *Text* = natural language; *Vision* = image input/output; *Video* = video analysis/generation; *Code* = code generation/debugging; *UI* = computer UI control. Bold indicates notable new modality for that model.)  
*(OpenAI’s *o1*, *o3* refer to their specialized “observer” series models focused on step-by-step reasoning.)*

### OpenAI GPT-4o, GPT-4.5, and GPT-4 Turbo

**GPT-4o (“Omni”)** was OpenAI’s 2024 flagship model, effectively an improved GPT-4 with similar intelligence but significantly faster responses ([ChatGPT — Release Notes | OpenAI Help Center](https://help.openai.com/en/articles/6825453-chatgpt-release-notes#:~:text=GPT,more%20about%20the%20announcement%20here)). It introduced multimodal capabilities (text and image understanding) to all users and laid the groundwork for voice and tools integration ([ChatGPT — Release Notes | OpenAI Help Center](https://help.openai.com/en/articles/6825453-chatgpt-release-notes#:~:text=GPT,more%20about%20the%20announcement%20here)). Upon launch, GPT-4o matched GPT-4’s abilities but with notable enhancements: it followed complex instructions more faithfully, solved harder coding problems with cleaner code, and produced more *“intuitive, collaborative”* responses ([ChatGPT — Release Notes | OpenAI Help Center](https://help.openai.com/en/articles/6825453-chatgpt-release-notes#:~:text=We%E2%80%99ve%20made%20improvements%20to%20GPT,and%20a%20clearer%20communication%20style)) ([ChatGPT — Release Notes | OpenAI Help Center](https://help.openai.com/en/articles/6825453-chatgpt-release-notes#:~:text=GPT,higher%20accuracy%20in%20classification%20tasks)). In May 2024 it became the default model for ChatGPT Plus/Enterprise, with *5×* higher message limits than the old GPT-4 ([ChatGPT — Release Notes | OpenAI Help Center](https://help.openai.com/en/articles/6825453-chatgpt-release-notes#:~:text=Currently%2C%20only%20the%20new%20text,more%20about%20the%20announcement%20here)). GPT-4o also expanded context length to 128,000 tokens in the API (allowing ~300 pages of text in one prompt) ([GPT-4 Turbo in the OpenAI API | OpenAI Help Center](https://help.openai.com/en/articles/8555510-gpt-4-turbo-in-the-openai-api#:~:text=GPT,for%20this%20model%20is%204096)). This huge context window enables working with lengthy documents or transcripts seamlessly. GPT-4o supports function calling and plugins, meaning it can invoke external APIs or run code as part of its answers.

To bring GPT-4-level AI to smaller devices and budgets, OpenAI released **GPT-4o Mini** in mid-2024. Despite a much smaller parameter count, GPT-4o Mini *“surpasses GPT-3.5 Turbo and other small models”* on academic benchmarks ([ChatGPT — Release Notes | OpenAI Help Center](https://help.openai.com/en/articles/6825453-chatgpt-release-notes#:~:text=We%E2%80%99re%20introducing%20GPT,3.5%20Turbo)). It retains multimodal reasoning (e.g. vision understanding) and strong function-calling ability ([ChatGPT — Release Notes | OpenAI Help Center](https://help.openai.com/en/articles/6825453-chatgpt-release-notes#:~:text=We%E2%80%99re%20introducing%20GPT,3.5%20Turbo)). This model gives developers an affordable option to leverage advanced GPT-4-class performance in their own apps.

**GPT-4 Turbo** (2024) was an enhanced version of GPT-4 focusing on efficiency. It offered the same base knowledge and skill as GPT-4, but at a fraction of the cost (about 3× cheaper inputs) and with the full 128k context window ([GPT-4 Turbo in the OpenAI API | OpenAI Help Center](https://help.openai.com/en/articles/8555510-gpt-4-turbo-in-the-openai-api#:~:text=GPT,for%20this%20model%20is%204096)) ([GPT-4 Turbo in the OpenAI API | OpenAI Help Center](https://help.openai.com/en/articles/8555510-gpt-4-turbo-in-the-openai-api#:~:text=capable%2C%20has%20an%20updated%20knowledge,for%20this%20model%20is%204096)). This model was ideal for applications needing long-context understanding (e.g. analyzing lengthy contracts or codebases) without the original GPT-4’s latency. GPT-4 Turbo became available via API to all GPT-4 users, making large-context AI more accessible. *(Notably, OpenAI’s documentation notes GPT-4 Turbo had an updated knowledge cutoff of April 2023 ([GPT-4 Turbo in the OpenAI API | OpenAI Help Center](https://help.openai.com/en/articles/8555510-gpt-4-turbo-in-the-openai-api#:~:text=GPT,for%20this%20model%20is%204096)), indicating it was trained on more recent data than the original GPT-4.)*

**ChatGPT-4.5** (Feb 2025) represents OpenAI’s most advanced general model to date, bridging the gap before GPT-5. It was introduced as a **ChatGPT Pro** feature and API preview ([GPT-4.5 explained: Everything you need to know](https://www.techtarget.com/whatis/feature/GPT-45-explained-Everything-you-need-to-know#:~:text=At%20the%20time%20of%20its,based%20LLM)) ([GPT-4.5 explained: Everything you need to know](https://www.techtarget.com/whatis/feature/GPT-45-explained-Everything-you-need-to-know#:~:text=GPT,will%20roll%20out%20broader%20availability)). Architecturally, GPT-4.5 builds on GPT-4o with even more extensive pre-training and fine-tuning. OpenAI kept details sparse, but confirmed it is their largest model so far ([GPT-4.5 explained: Everything you need to know](https://www.techtarget.com/whatis/feature/GPT-45-explained-Everything-you-need-to-know#:~:text=OpenAI%20did%20not%20disclose%20the,and%20custom%20data%20developed%20internally)). Rather than emphasizing chain-of-thought reasoning, GPT-4.5 leans into unsupervised pretraining scale – it responds based on vast knowledge and pattern recognition, while leaving explicit step-by-step reasoning to specialized models (OpenAI’s “o-series”) ([GPT-4.5 explained: Everything you need to know](https://www.techtarget.com/whatis/feature/GPT-45-explained-Everything-you-need-to-know#:~:text=GPT,math%2C%20science%20and%20logic%20problems)) ([GPT-4.5 explained: Everything you need to know](https://www.techtarget.com/whatis/feature/GPT-45-explained-Everything-you-need-to-know#:~:text=DeepSeek%20R1%2C%20which%20use%20chain,math%2C%20science%20and%20logic%20problems)). This makes GPT-4.5 a highly generalist AI: extremely knowledgeable and smooth in conversation, though not as slow-deliberate as the reasoning specialists. In practice, GPT-4.5 produces more **factual and concise answers** than its predecessors – it *“hallucinates”* (makes up facts) far less ([Introducing GPT-4.5 | OpenAI](https://openai.com/index/introducing-gpt-4-5/#:~:text=often%20drew%20on%20classical%20themes,stories%20from%20antiquity%20for%20inspiration)) and knows when to stop and ask if the user needs more, thanks to training for higher emotional intelligence ([Introducing GPT-4.5 | OpenAI](https://openai.com/index/introducing-gpt-4-5/#:~:text=If%20you%20need%20further%20support,better%20with%20time%20and%20effort)) ([Introducing GPT-4.5 | OpenAI](https://openai.com/index/introducing-gpt-4-5/#:~:text=GPT%E2%80%914,less%20than%20other%20OpenAI%20models)). It can handle nuanced dialogues, offering empathy or advice in an *“EQ”-aware* manner ([Introducing GPT-4.5 | OpenAI](https://openai.com/index/introducing-gpt-4-5/#:~:text=If%20you%20need%20further%20support,better%20with%20time%20and%20effort)). GPT-4.5 also supports **multimodal inputs** like image and file uploads for analysis ([Introducing GPT-4.5 | OpenAI](https://openai.com/index/introducing-gpt-4-5/#:~:text=users%20next%20week%2C%20then%20to,Edu%20users%20the%20following%20week)), and has built-in browsing/search access to provide up-to-date info ([Introducing GPT-4.5 | OpenAI](https://openai.com/index/introducing-gpt-4-5/#:~:text=Starting%20today%2C%20ChatGPT%20Pro%20users,Edu%20users%20the%20following%20week)) (a user no longer needs a plugin – GPT-4.5 can fetch information live). It integrates with a *Canvas* tool for brainstorming – able to write and edit in a shared document or whiteboard space ([Introducing GPT-4.5 | OpenAI](https://openai.com/index/introducing-gpt-4-5/#:~:text=How%20to%20use%20GPT,ChatGPT)). However, at launch it did **not** enable ChatGPT’s voice or vision-output modes (those remained on GPT-4o) ([Introducing GPT-4.5 | OpenAI](https://openai.com/index/introducing-gpt-4-5/#:~:text=GPT%E2%80%914,AI%20%E2%80%9Cjust%20works%E2%80%9D%20for%20you)). GPT-4.5 focuses on text and knowledge tasks, leaving multimodal generation to other models for now. Early benchmarks show it **improved creative writing and conversational flow**, feeling more like an expert human tutor or colleague. For instance, GPT-4.5’s answers tend to be structured and to-the-point, whereas GPT-4o might be more verbose ([Introducing GPT-4.5 | OpenAI](https://openai.com/index/introducing-gpt-4-5/#:~:text=Stronger%20reasoning%20on%20the%20horizon)) ([Introducing GPT-4.5 | OpenAI](https://openai.com/index/introducing-gpt-4-5/#:~:text=approaches%20to%20scaling%E2%80%94pre,using%20agents)). OpenAI noted that 4.5 was trained with new reinforcement learning techniques and an *“instruction hierarchy”* to better follow system rules over user prompts ([GPT-4.5 explained: Everything you need to know](https://www.techtarget.com/whatis/feature/GPT-45-explained-Everything-you-need-to-know#:~:text=%2A%20Supervised%20fine,messages%20over%20conflicting%20user%20instructions)) ([GPT-4.5 explained: Everything you need to know](https://www.techtarget.com/whatis/feature/GPT-45-explained-Everything-you-need-to-know#:~:text=and%20taught%20it%20to%20follow,instructions%20to%20prevent%20prompt%20injection)) – an effort to make it resistant to prompt injection attacks. Overall, GPT-4.5 is the current pinnacle of OpenAI’s strategy of scaling up pretraining to make a generally smarter model ([Introducing GPT-4.5 | OpenAI](https://openai.com/index/introducing-gpt-4-5/#:~:text=GPT%E2%80%914,using%20agents)). It serves as a robust foundation that can later be augmented with explicit reasoning modules or tool-use, as OpenAI hints that *“future models will combine pre-training and reasoning”* approaches ([Introducing GPT-4.5 | OpenAI](https://openai.com/index/introducing-gpt-4-5/#:~:text=GPT%E2%80%914,using%20agents)).

OpenAI’s strategy in 2024–2025 clearly bifurcated into **general-purpose large models (GPT-4.5)** and **specialized “reasoners” (the o-series)**. The **OpenAI o-series** (observer models) like **o1** and **o3** are designed to “think for longer before responding” and use chain-of-thought to tackle complex problems ([Introducing OpenAI o3 and o4-mini | OpenAI](https://openai.com/index/introducing-o3-and-o4-mini/#:~:text=Today%2C%20we%E2%80%99re%20releasing%20OpenAI%20o3,For%20the)). For example, OpenAI o3 (released alongside GPT-4o Mini) was described as *“our most powerful reasoning model... ideal for complex queries requiring multi-step logic”* ([Introducing OpenAI o3 and o4-mini | OpenAI](https://openai.com/index/introducing-o3-and-o4-mini/#:~:text=match%20at%20L152%20OpenAI%20o3,for%20complex%20queries%20requiring%20multi)). It uses an RL-trained approach to reason through math proofs, code, and scientific problems. These models can use tools and calculators explicitly within their reasoning steps. OpenAI even gave o3 full tool access in ChatGPT (browsing, code execution, etc.) and trained it to decide **when** to invoke a tool ([Introducing OpenAI o3 and o4-mini | OpenAI](https://openai.com/index/introducing-o3-and-o4-mini/#:~:text=match%20at%20L235%20OpenAI%20o3,in%20the%20right%20output%20formats)). This effectively makes models like o3 early instances of agentic AI within the chat interface. However, the o-series models are smaller and slower than GPT-4.5, so OpenAI positions them for tasks where meticulous logic is needed, whereas GPT-4.5 is the fast knowledge expert ([Introducing GPT-4.5 | OpenAI](https://openai.com/index/introducing-gpt-4-5/#:~:text=GPT%E2%80%914,using%20agents)). The interplay of GPT-4.5 with these reasoning agents is an emerging paradigm (OpenAI suggests future models will unify these capabilities).

### Anthropic Claude 3.5 (“Sonnet”) and Beyond

Anthropic’s **Claude 3.5 “Sonnet”** (late 2024) marked a significant leap for the Claude series. Claude 2 (July 2023) was already notable for a 100k token context and strong language abilities; Claude 3.5 extends this with *agentic* and coding-oriented features. Nicknamed *“Sonnet”*, this model was tuned for **real-world software engineering tasks and tool use** ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Claude%203,computer%20use%20in%20public%20beta)). It can write complex code, debug, and even control computer interfaces in a limited way (Anthropic demonstrated Claude Sonnet operating a virtual computer environment – termed “computer use” – to carry out multi-step tasks) ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Claude%203,computer%20use%20in%20public%20beta)) ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Hybrid%20reasoning%20model%2C%20state,use%2C%20and%20200K%20context%20window)). Sonnet introduced **Conversational Speech Model integration**, meaning it can produce or understand speech as part of its API (e.g. for voice assistants), and it can handle **multimodal input** to some extent (e.g. interpreting an image or diagram if provided in a prompt, though Anthropic’s emphasis remained on text and code). Claude 3.5’s standout feature is *“agentic capabilities”*: it doesn’t just follow a single prompt, but can autonomously break down a goal into sub-tasks, invoke tools or APIs (Anthropic partners integrated Claude into systems like Slack with plugins), and carry on long-running sessions with persistent memory. Anthropic reported that Claude 3.5 Sonnet can even use a web browser and keyboard – for example, one demo had Claude Sonnet taking actions in a simulated browser to research and gather information for the user. This blurs the line between a static chatbot and a proactive *AI assistant*.

Technically, Claude 3.5 introduced a **hybrid architecture** under the hood. The Anthropic team describes models in the Claude 3 family with codenames like *Opus*, *Sonnet*, *Haiku* ([[PDF] The Claude 3 Model Family: Opus, Sonnet, Haiku - Anthropic](https://www.anthropic.com/claude-3-model-card#:~:text=,IFEval)). Sonnet is specialized for coding and “agent” behavior, likely meaning it was further fine-tuned on code execution traces and tool-use demonstrations. It still has Anthropic’s core safety and constitutional AI principles, but with more flexibility to decide how to help the user achieve a goal (even if it requires external actions). It maintained Claude’s hallmark of extremely long context – at least 100,000 tokens. By early 2025, Anthropic released **Claude 3.7 Sonnet**, which they call the first *“hybrid reasoning model”* ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Feb%2024%2C%202025)). This model can operate in two modes: a fast, near-instant mode for straightforward prompts (like drafting an email), and a slower, step-by-step mode for complex problem-solving ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Feb%2024%2C%202025)) ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Claude%203,a%20variety%20of%20use%20cases)). Developers can explicitly toggle how long Claude “thinks,” even mid-conversation. This is analogous to having a quick intuitive brainstorm versus a deep analytical session. Claude 3.7 also expanded its multimodal abilities (vision understanding improved, though not fully public) and context window to **~200K tokens** ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Hybrid%20reasoning%20model%2C%20state,use%2C%20and%20200K%20context%20window)), surpassing its predecessors. In coding, Claude 3.7 is state-of-the-art, often outperforming other models on code benchmarks while also being able to reason about the purpose of the code (thanks to the hybrid approach). 

In practical use, Claude models are available via API (including on platforms like Amazon Bedrock and Google’s Vertex AI) and have been integrated into products like **Snowflake’s** machine learning offering ([Anthropic's Claude 3.5 Sonnet now available in Snowflake Cortex AI](https://www.snowflake.com/en/blog/anthropic-claude-sonnet-cortex-ai/#:~:text=Anthropic%27s%20Claude%203,about%20generative%20AI%20in)). Students and researchers have lauded Claude 3.x for its ability to handle entire research papers as input and give a detailed summary or critique in one go, courtesy of its massive context. The combination of large context and agentic planning makes Claude 3.5/3.7 a sort of “AI project manager” that can consume all relevant information and then outline a plan of action.

One example of Claude’s agentic behavior: given a high-level task *“Analyze the sentiment of the past week’s news and plot a graph”*, Claude Sonnet could search news articles (via a browsing plugin), summarize the sentiment, write a Python script to generate a sentiment timeline, execute it (through an execution tool), and then return the resulting chart – all in one conversation. This showcases the emerging *AI researcher* role these models can take.

### Google DeepMind Gemini: Pro and Flash Models

Google’s **Gemini** is a next-generation foundation model announced in late 2023 and gradually rolled out through 2024–2025. By combining Google Brain’s large-model expertise (e.g. PaLM 2) with DeepMind’s reinforcement learning prowess, Gemini aims to be a **multimodal, “thinking” AI** competing with GPT-4 and beyond. In 2024, Google released **Gemini 2.0 Flash** as an early preview: this was a smaller variant optimized for speed and cost efficiency in serving, hence the *“Flash”* name. Gemini 2.0 Flash could handle natural language and some code, with reasonable reasoning abilities, but its main draw was **fast response** and low compute requirements – suitable for high-volume applications. 

The true leap came with **Gemini 2.5** in early 2025. Google introduced two tiers: **Gemini 2.5 Pro** (the full-scale model) and **Gemini 2.5 Flash Preview** (a mid-scale model for developer preview) ([Our latest AI models - Google AI](https://ai.google/get-started/our-models/#:~:text=Our%20latest%20AI%20models%20,Balance%20performance%20and%20your%20budget)) ([Gemini models | Gemini API | Google AI for Developers](https://ai.google.dev/gemini-api/docs/models#:~:text=Gemini%20models%20,as%20analyzing%20large%20datasets%2C)). Both versions are **built with reasoning capabilities “natively” integrated** – DeepMind has stated that Gemini models reason through problems by internally simulating thought chains (“let’s think this through”) before final answers ([Gemini - Google DeepMind](https://deepmind.google/technologies/gemini/#:~:text=Gemini%20,enhanced%20performance%20and%20improved%20accuracy)). In fact, Gemini 2.5 models perform chain-of-thought prompting under the hood: they were trained to generate explicit reasoning steps, but these steps can be hidden or toggled for speed. This leads to the concept of **“fully hybrid reasoning”**: as noted, the Gemini 2.5 Flash model allows developers to *turn on/off* the heavy reasoning mode ([
            
            Start building with Gemini 2.5 Flash
            
            
            - Google Developers Blog
            
        ](https://developers.googleblog.com/en/start-building-with-gemini-25-flash/#:~:text=match%20at%20L150%20still%20prioritizing,with%20thinking%20off%2C%20developers%20can)). With *thinking off*, Gemini Flash behaves like a quick response model (similar to ChatGPT’s fast mode), and with *thinking on*, it will deliberately take a few extra seconds to produce a more detailed, logical answer (similar to an “analytical mode”). Developers can even set a *“thinking budget”*, e.g. allowing the model 2 seconds to reason, to balance latency vs. quality ([
            
            Start building with Gemini 2.5 Flash
            
            
            - Google Developers Blog
            
        ](https://developers.googleblog.com/en/start-building-with-gemini-25-flash/#:~:text=still%20prioritizing%20speed%20and%20cost,with%20thinking%20off%2C%20developers%20can)). This is an innovative feature that effectively puts the throttle of reasoning in the user’s hands.

**Gemini 2.5 Pro**, the flagship, is a much larger model (reported to have hundreds of billions of parameters, possibly with Mixture-of-Experts) and *achieved state-of-the-art results* on many benchmarks in late 2024 ([Gemini Pro - Google DeepMind](https://deepmind.google/technologies/gemini/pro/#:~:text=Gemini%202)). It excels at complex coding tasks, math word problems, and multimodal understanding. Google’s technical report noted it significantly outperformed GPT-4 on certain reasoning benchmarks ([Introducing OpenAI o3 and o4-mini | OpenAI](https://openai.com/index/introducing-o3-and-o4-mini/#:~:text=OpenAI%20o3%20is%20our%20most,for%20complex%20queries%20requiring%20multi)). It’s *“our most advanced coding model yet,”* DeepMind wrote, and is *state-of-the-art across a range of benchmarks requiring enhanced reasoning* ([Gemini Pro - Google DeepMind](https://deepmind.google/technologies/gemini/pro/#:~:text=match%20at%20L461%20Gemini%202,of%20benchmarks%20requiring%20enhanced%20reasoning)). Gemini Pro also has multimodal abilities: it was designed from the ground up to handle not just text, but also images and possibly other inputs. For example, Gemini can describe an image or answer questions about it (much like GPT-4 Vision). There are hints that video understanding is in the works too, given DeepMind’s expertise in video models – though as of 2.5, image and text are the primary modalities exposed. Its training involved using a **large memory of tools and APIs**, as Google integrated their Toolformer research. Indeed, *Gemini was demonstrated controlling Google products (like issuing queries or actions in spreadsheets via API calls)*, although the full tool-use API hasn’t been publicly launched yet.

One striking aspect: Google claims Gemini **reasoned through code to create simulations and games** from single-line prompts ([Gemini Pro - Google DeepMind](https://deepmind.google/technologies/gemini/pro/#:~:text=match%20at%20L166%20Watch%20Gemini,from%20a%20single%20line%20prompt)) ([Gemini Pro - Google DeepMind](https://deepmind.google/technologies/gemini/pro/#:~:text=Watch%20Gemini%202,and%20health%20indicators%20over%20time)). In demos, *Gemini 2.5 Pro wrote an interactive “cosmic fish” animation from a simple prompt* ([Gemini Pro - Google DeepMind](https://deepmind.google/technologies/gemini/pro/#:~:text=See%20how%20Gemini%202,simulations%20and%20do%20advanced%20coding)), or created a working **endless runner game** using JavaScript, just from a high-level request ([Gemini Pro - Google DeepMind](https://deepmind.google/technologies/gemini/pro/#:~:text=match%20at%20L166%20Watch%20Gemini,from%20a%20single%20line%20prompt)). These showcase its capability to combine logic, coding, and understanding of physics or visuals to produce dynamic results. This kind of “AI coder + thinker” in one is a leap beyond earlier code assistants.

Under the hood, Gemini leverages **Mixture-of-Experts (MoE)** in at least some variants. The Qwen team’s evaluation mentions *“Llama-3.1-405B, the largest open-weight dense model”* and compares it to *DeepSeek V3 (leading open-weight MoE) and Qwen2.5-72B* ([Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen](https://qwenlm.github.io/blog/qwen2.5-max/#:~:text=When%20comparing%20base%20models%2C%20we,this%20comparison%20are%20presented%20below)) – implying that DeepSeek V3 and perhaps Gemini are MoE. MoE allows models to scale to effectively trillions of parameters by activating only relevant “experts” per query. This approach likely helped Gemini 2.5 Pro achieve top performance without an excessive increase in latency.

In terms of availability, Gemini 2.5 was offered through Google’s Vertex AI as a limited preview (with researchers and select partners gaining access first). The **Google Developers Blog** announced the opening of *Gemini 2.5 Flash in preview on April 17, 2025* ([
            
            Start building with Gemini 2.5 Flash
            
            
            - Google Developers Blog
            
        ](https://developers.googleblog.com/en/start-building-with-gemini-25-flash/#:~:text=Start%20building%20with%20Gemini%202,Flash)), inviting developers to test it via the Gemini API. The Pro model was moving from “Experimental” to “Preview” around the same time, with increased rate limits for those approved ([Gemini Pro - Google DeepMind](https://deepmind.google/technologies/gemini/pro/#:~:text=Start%20testing%20Gemini%202)). Google also published a detailed technical report (though some experts noted it *“lacks key safety details”* ([Google's latest AI model report lacks key safety details, experts say](https://techcrunch.com/2025/04/17/googles-latest-ai-model-report-lacks-key-safety-details-experts-say/#:~:text=Google%27s%20latest%20AI%20model%20report,safety%20details%2C%20according%20to%20experts))). Nonetheless, external evaluations show Gemini 2.5 Pro to be on par with the best models in the world on reasoning and coding, and notably strong in **multilingual understanding** and **scientific knowledge** (benefiting from Google’s vast training data). One unique advantage is Google’s integration of Gemini with its ecosystem: e.g., if you use Bard (Google’s chat service), it’s expected to be powered by a scaled version of Gemini, meaning it can seamlessly incorporate search results, maps, or other Google tools as part of answers.

### Meta’s LLaMA 3/4: Scout and Maverick

Meta’s open-source LLM journey progressed rapidly. After LLaMA 2 (70B) in 2023, Meta AI released what the community initially dubbed **LLaMA 3**, but Meta officially named it **LLaMA 4** – a new family of models including **LLaMA 4 Scout** and **LLaMA 4 Maverick** (with an even larger **“Behemoth”** in the works) ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=Now%20we%20know%20the%20fruits,code%20sharing%20community%20Hugging%20Face)) ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=A%20massive%202,and%20complex%20all%20around%20model)). These models were announced by Meta’s CEO in early 2025 and made openly available for download and use ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=new%20Llama%204%20series%20of,code%20sharing%20community%20Hugging%20Face)), continuing Meta’s commitment to open development. 

**LLaMA 4 Scout** is a relatively smaller model in terms of *active* parameters – about 17B parameters per expert – but it uses a **Mixture-of-Experts with 128 experts**, which means the total parameter count is much higher (Scout’s *total* capacity was reported as ~109B) ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=Now%20we%20know%20the%20fruits,code%20sharing%20community%20Hugging%20Face)). **LLaMA 4 Maverick** is larger, with ~400B total parameters (128 experts as well) ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=Now%20we%20know%20the%20fruits,code%20sharing%20community%20Hugging%20Face)). Importantly, both are **multimodal** and **natively support extremely long context lengths**. Scout can handle a context window of up to **10 million tokens** (yes, 10,000,000) ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=One%20headline%20feature%20of%20these,audio%20was%20not%20mentioned)), and Maverick up to **1 million tokens** ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=One%20headline%20feature%20of%20these,audio%20was%20not%20mentioned)). These context sizes are orders of magnitude beyond other models – for perspective, 10M tokens is roughly 7,500 pages of text! This was achieved by a combination of sparse attention mechanisms and the MoE architecture (which reduces the memory per token). With such context, a user could literally feed an entire book series or massive dataset into LLaMA 4 Scout and ask questions that require cross-referencing anywhere in those documents.

The LLaMA 4 series are also **fully multimodal**: they were trained not only on text but also on images and video frames ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=generally%20more%20mean%20a%20more,and%20complex%20all%20around%20model)). In Meta’s announcement, they highlighted that LLaMA 4 models *“are capable of receiving and generating text, video, and imagery”*, although they did not mention audio ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=generally%20more%20mean%20a%20more,and%20complex%20all%20around%20model)). This makes them the first open-source models to jointly handle text and video. For example, LLaMA 4 could take as input a short video clip and generate a textual description or continue the video (in the form of generating video frames) – capabilities that were previously limited to proprietary research models.

Another major innovation: LLaMA 4 models use a **diffusion-based architecture for context processing** (as hinted in some research papers) and heavy use of retrieval. They employ *128 experts* that specialize in different domains or skills ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=All)). When a query comes in, only a subset of the experts activate, making it efficient. Meta’s system card notes that *“only a subset of total parameters are activated for each token, improving efficiency”* ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=All)). This MoE approach also allowed Meta to train these giant models relatively quickly (since each training instance only updates certain experts). The result is an *open model that matches or surpasses the capabilities of similarly sized closed models*. In fact, Meta explicitly positioned LLaMA 4 as an **answer to DeepSeek R1 and Google/Anthropic** in the open arena. VentureBeat reported that *“Meta’s answer to DeepSeek is here: LLaMA 4 launches… beating frontier labs to the punch”* by open-sourcing frontier-level tech ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=measured%20by%20X%2C%20at%20least,a%20coding%20competition)). 

Concretely, **LLaMA 4 Maverick** (400B) is extremely good at knowledge-intensive tasks and reasoning – it has so many parameters and experts that each expert can be thought of as a specialist (some might be experts in coding, some in law, etc.). It achieved leading scores on open benchmarks, and because Meta released the weights, the community rapidly started fine-tuning and building on it. We have already seen community-driven enhancements like instruction-tuned versions and low-RAM distillations. Meanwhile, **LLaMA 4 Scout** (109B) with its 10M context is a game-changer for working with large data: researchers can feed an entire corpus (say all Wikipedia) into a single prompt in chunks and ask extremely broad or detailed questions. Scout’s multimodal ability also means one can input, for example, a scientific paper with embedded figures and get an analysis that references the figures.

One fascinating aspect: due to MoE, **the models are relatively *efficient to run***. While storing 400B parameters is heavy, at inference time each token only uses a fraction of those. Meta and NVIDIA even collaborated to ensure these models run on GPU clusters efficiently ([NVIDIA Accelerates Inference on Meta Llama 4 Scout and Maverick](https://developer.nvidia.com/blog/nvidia-accelerates-inference-on-meta-llama-4-scout-and-maverick/#:~:text=NVIDIA%20Accelerates%20Inference%20on%20Meta,source)). NVIDIA announced optimizations for LLaMA 4 on their inference platforms, noting near-real-time generation speeds for Maverick on high-end hardware ([NVIDIA Accelerates Inference on Meta Llama 4 Scout and Maverick](https://developer.nvidia.com/blog/nvidia-accelerates-inference-on-meta-llama-4-scout-and-maverick/#:~:text=NVIDIA%20Accelerates%20Inference%20on%20Meta,source)).

In summary, Meta’s LLaMA 3/4 (Scout & Maverick) push the frontier of open-source: *fully multimodal, ultra-long-context, expert-level LLMs* that anyone can deploy. This has big implications for academia and industry – it reduces reliance on API access to closed models. For instance, a university can set up LLaMA 4 Maverick on their servers and students can query a vast video & text library for research, all locally. It’s worth noting that Meta also previewed **LLaMA 4 “Behemoth”** (not yet released as of early 2025), a 2-trillion parameter MoE model still in training ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=Face%20huggingface)). If and when Behemoth arrives, it would be the largest AI model publicly known. Meta hints that Behemoth will handle even more complex tasks and could potentially incorporate audio too, aiming at a truly universal model of “all of life’s modalities.”

### DeepSeek R1 and Open Reasoning Models

**DeepSeek R1** made headlines in January 2025 as the first openly released model to *match the performance of OpenAI’s top reasoning model (o1)* ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=measured%20by%20X%2C%20at%20least,a%20coding%20competition)). Developed by the Chinese AI firm DeepSeek, **R1** is specialized for complex reasoning in math, coding, and science. It uses a unique training regimen: a large base model (DeepSeek V3, 671B parameters, using mixture-of-experts) was fine-tuned with **reinforcement learning to perform chain-of-thought reasoning** ([DeepSeek explained: Everything you need to know](https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know#:~:text=challenges.%20%2A%20DeepSeek,a%20context%20length%20of%20128%2C000)) ([DeepSeek explained: Everything you need to know](https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know#:~:text=%2A%20DeepSeek,a%20context%20length%20of%20128%2C000)). Essentially, DeepSeek trained R1 to “think step by step” and not just answer directly. The result was a model that on benchmarks like graduate-level QA (GPQA), advanced math competitions (AIME), and competitive programming problems (Codeforces), *“matches or comes close to matching”* OpenAI’s dedicated reasoning model ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=o1%E2%80%94beating%20frontier%20labs%20Anthropic%2C%20Google%E2%80%99s,a%20coding%20competition)). This was a remarkable achievement, considering DeepSeek is a startup founded in 2023, operating without access to the very latest Western GPUs due to export controls ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=than%20the%20cost%20of%20OpenAI%E2%80%99s,competitor%2C%20o1)) ([DeepSeek explained: Everything you need to know](https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know#:~:text=,with%20OpenAI%2C%20which%20is%20proprietary)).

DeepSeek R1’s open release was accompanied by a technical paper on arXiv detailing their methods ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=What%E2%80%99s%20more%2C%20DeepSeek%20released%20the,apps%20like%20Gemini%20and%20Claude)). They also open-sourced the model weights (minus the proprietary training data) under an open license ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=What%E2%80%99s%20more%2C%20DeepSeek%20released%20the,apps%20like%20Gemini%20and%20Claude)). Within days, the AI community had R1 running on local hardware and available on cloud platforms (AWS added DeepSeek-R1 to their bedrock marketplace ([DeepSeek-R1 models now available on AWS](https://aws.amazon.com/blogs/aws/deepseek-r1-models-now-available-on-aws/#:~:text=DeepSeek,Oregon%29%20AWS%20Regions))). The model’s impact was such that the DeepSeek app topped Apple’s App Store, briefly surpassing ChatGPT in downloads in China ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=much%20of%20the%20methodology%20needed,apps%20like%20Gemini%20and%20Claude)). 

The **DeepSeek model lineup** prior to R1 set the stage: 
- *DeepSeek LLM V1* was first released Dec 2023 as a general 30B-class model; 
- *DeepSeek V2* (May 2024) improved performance and efficiency; 
- *DeepSeek V3* (Dec 2024) introduced a 671B-parameter MoE model with 128k token context ([DeepSeek explained: Everything you need to know](https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know#:~:text=challenges.%20%2A%20DeepSeek,a%20context%20length%20of%20128%2C000)). V3 itself was a powerhouse, and R1 builds on it with reasoning optimization. 
- They also released domain-specific models like *DeepSeek Coder V2 (July 2024)*, a 236B model for coding with 128k context ([DeepSeek explained: Everything you need to know](https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know#:~:text=%2A%20DeepSeek,V3)). 
- Notably, DeepSeek V3 and R1 both have **128,000 token context windows** ([DeepSeek explained: Everything you need to know](https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know#:~:text=experts%20%20architecture%2C%20capable%20of,a%20context%20length%20of%20128%2C000)) – extremely high, enabling them to handle lengthy code files or multiple documents at once.

DeepSeek R1’s **distinct strength** is solving problems that require multiple steps of deduction or calculation. For example, it can take a complex math word problem, internally break it down into lemmas, and arrive at a correct answer where a standard GPT might falter. It also demonstrated an ability to write correct code for competitive programming challenges *without* human hints – something only OpenAI’s code-dedicated models had been able to do prior ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=measured%20by%20X%2C%20at%20least,a%20coding%20competition)). 

The open-source nature means anyone can incorporate R1 into their workflows. Its cost-efficiency is a major selling point: DeepSeek claims R1 was trained for under $6 million ([DeepSeek explained: Everything you need to know](https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know#:~:text=,with%20OpenAI%2C%20which%20is%20proprietary)), and running it in the cloud is much cheaper than using proprietary APIs ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=Alongside%20the%20main%20r1%20model%2C,cost%20of%20OpenAI%E2%80%99s%20competitor%2C%20o1)). In fact, the Lawfare analysis noted R1’s inference cost is ~**27× lower** than OpenAI o1’s cost via API ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=and%20far%20ahead%20of%20competitor,apps%20like%20Gemini%20and%20Claude)). This cost disruption has strategic implications – it prompted discussions in the U.S. about competitiveness and even caused a brief dip in AI companies’ stock prices as investors reassessed the “moat” of the closed models ([DeepSeek explained: Everything you need to know](https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know#:~:text=Listen%20to%20this%20article,audio%20was%20generated%20by%20AI)) ([DeepSeek explained: Everything you need to know](https://www.techtarget.com/whatis/feature/DeepSeek-explained-Everything-you-need-to-know#:~:text=But%20Chinese%20AI%20development%20firm,54%20license%2C%20enabling%20free%20use)).

Alongside R1, DeepSeek provided **smaller distilled versions** that can run on consumer hardware ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=and%20far%20ahead%20of%20competitor,apps%20like%20Gemini%20and%20Claude)). So one can use a laptop to run a ~13B distilled reasoning model that still outperforms many 30B models, thanks to the distilled knowledge from R1. This further democratizes access.

**Qwen models (Alibaba)** are another important open family. Alibaba’s Qwen (通义千问) models, first open-sourced in 2023 (7B and 14B) and later a 72B model ([Qwen - Wikipedia](https://en.wikipedia.org/wiki/Qwen#:~:text=by%20Meta%20AI.,7)), saw an upgrade in 2024. **Qwen-2** introduced Mixture-of-Experts and a model called **Qwen-2.5 Max**, which was made accessible via API in January 2025 ([Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen](https://qwenlm.github.io/blog/qwen2.5-max/#:~:text=extremely%20large%20models%2C%20whether%20they,Max%20on%20%205%20Qwen)) ([Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen](https://qwenlm.github.io/blog/qwen2.5-max/#:~:text=Today%2C%20we%20are%20excited%20to,Max%20on%20Qwen%20Chat)). Qwen-2.5 Max is an MoE model trained on an enormous 20 trillion tokens of data ([Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen](https://qwenlm.github.io/blog/qwen2.5-max/#:~:text=extremely%20large%20models%2C%20whether%20they,Max%20on%20%205%20Qwen)), giving it a very broad and deep knowledge base. Alibaba reported that Qwen-2.5 Max **outperforms DeepSeek V3** on a range of benchmarks (like knowledge tests, code benchmarks, etc.) ([Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen](https://qwenlm.github.io/blog/qwen2.5-max/#:~:text=including%20DeepSeek%20V3%2C%20GPT,Sonnet)) ([Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen](https://qwenlm.github.io/blog/qwen2.5-max/#:~:text=Image)), and is competitive with the best proprietary models. While the full Qwen-2.5 Max is available through Alibaba Cloud (it’s not fully open-weight to download), Alibaba did open-source smaller models like Qwen-2.5-72B (a dense 72B that is among the top open models) ([Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen](https://qwenlm.github.io/blog/qwen2.5-max/#:~:text=When%20comparing%20base%20models%2C%20we,this%20comparison%20are%20presented%20below)) and even a *reasoning-specialized* model called **Qwen-QwQ-32B** – a preview model focusing on chain-of-thought reasoning similar to OpenAI o1 ([Qwen - Wikipedia](https://en.wikipedia.org/wiki/Qwen#:~:text=In%20November%202024%2C%20QwQ,13)). Qwen-QwQ-32B was released under Apache 2.0 in Nov 2024 with 32k context and was shown to beat OpenAI o1 on some reasoning benchmarks ([Qwen - Wikipedia](https://en.wikipedia.org/wiki/Qwen#:~:text=In%20November%202024%2C%20QwQ,13)). This mirrors the trend we’ve seen: Chinese companies have put significant emphasis on high-quality reasoning and open-sourcing.

One should not overlook that Alibaba’s Qwen also has strong **multilingual capabilities** (it was ranked #1 on Chinese language benchmarks in July 2024 ([Qwen - Wikipedia](https://en.wikipedia.org/wiki/Qwen#:~:text=Qwen%20,1))) and **vision models** (Qwen-VL and Qwen-VL2 for image understanding ([Qwen - Wikipedia](https://en.wikipedia.org/wiki/Qwen#:~:text=The%20Qwen,sold%20by%20%2058%20at)), including a flagship vision model Qwen-VL-Max). The Qwen ecosystem is essentially China’s parallel to LLaMA and GPT – providing open models for both text and images. 

In summary, **open-source LLMs in 2024–2025 have reached parity with or exceeded closed models** on many fronts. From DeepSeek’s reasoning dominance to Meta’s massive context models and Alibaba’s MoE advancements, the community and industry now have access to cutting-edge model weights. This enables hybrid systems (e.g., using GPT-4.5 for general chat but a local LLaMA-4 for processing proprietary data) and gives researchers freedom to inspect and improve these models. The frontier of language models is no longer only behind corporate APIs – it’s increasingly on Hugging Face and GitHub, which is a major development for transparency and innovation.

## Audio and Voice Generation Models

Text-to-speech and audio generation AIs have made dramatic progress in realism and flexibility. As of 2024–2025, we have models that can produce lifelike speech, clone voices from a few seconds of sample audio, generate expressive dialog or even create original music and singing. Below is a summary of notable audio/voice generation models and tools:

- **Suno’s Bark (and Suno AI models):** *Suno AI* open-sourced “**Bark**,” a transformer-based text-to-audio model, in 2023. By 2024, Bark and its variants became popular for free, high-quality voice generation. Bark can generate highly realistic speech in **multiple languages**, complete with natural prosody and even non-verbal sounds (it can insert laughter, sighs, etc., as appropriate) ([suno-ai/bark: Text-Prompted Generative Audio Model - GitHub](https://github.com/suno-ai/bark#:~:text=Bark%20is%20a%20transformer,as%20well%20as%20other%20audio)) ([Bark - a Hugging Face Space by suno](https://huggingface.co/spaces/suno/bark#:~:text=Bark%20can%20generate%20highly%20realistic%2C,noise%20and%20simple%20sound%20effects)). Uniquely, Bark does **not** require a predetermined voice – it can output a variety of voices or mimic a given speaker token, making it versatile. It even produces music or background noises when prompted, something most TTS models avoid ([Bark - a Hugging Face Space by suno](https://huggingface.co/spaces/suno/bark#:~:text=Bark%20,noise%20and%20simple%20sound%20effects)). The audio Bark produces is notably natural in tone; it captures the nuances like pacing and intonation in a way earlier TTS systems struggled with. Bark’s **strengths** lie in its openness and creativity: one can prompt it not just with text but also with instructions like “[clears throat]” or “[speaking in French]” and it will obey. Use cases include experimental voice-overs, indie video game characters, or adding voice to hobby projects without licensing a commercial API. Bark’s **realism** is very good, though still a hair behind top closed models in clarity. Its latency is higher (being a large transformer), but ongoing work by the Suno team and community has improved speed. By 2025, Bark has become the *de facto* free TTS for hackers, and people have even built one-click apps to run it locally ([Free Text-to-Speech | SPARK TTS (Easy Installation) - YouTube](https://www.youtube.com/watch?v=kswY1uMN90g#:~:text=Free%20Text,For%20windows)).

- **Udio (AI Music Generator):** **Udio** is a system focused on *generative music with vocals*. Launched in beta in April 2024 by a startup founded by ex-Google researchers ([Udio - Wikipedia](https://en.wikipedia.org/wiki/Udio#:~:text=Udio%20is%20a%20generative%20artificial,capabilities%20such%20as%20audio%20inpainting)) ([Udio - Wikipedia](https://en.wikipedia.org/wiki/Udio#:~:text=Founded%20in%20December%202023%20by,possibility%20that%20its%20%2066)), Udio acts as an **“AI co-composer”**. A user provides a simple text prompt – e.g. *“a pop song about summer, with a female vocal in the style of K-pop”* – and Udio will generate a short song complete with instrumental backing and sung lyrics ([Udio - Wikipedia](https://en.wikipedia.org/wiki/Udio#:~:text=Websiteudio)) ([Udio - Wikipedia](https://en.wikipedia.org/wiki/Udio#:~:text=Udio%20is%20a%20generative%20artificial,capabilities%20such%20as%20audio%20inpainting)). Under the hood, Udio has a generative model that produces both **melody and lyrics**, and a voice model to sing those lyrics in a specified style ([Udio - Wikipedia](https://en.wikipedia.org/wiki/Udio#:~:text=Websiteudio)). It can also generate instrumental music without vocals or allow users to input their own lyrics to be sung. Udio’s **unique capability** is producing coherent vocals: it was trained on a vast set of songs, learning how phonetics and melody intertwine. As a result, it often produces intelligible lyrics (or at least vocalizations that sound like singing) and keeps them in rhythm and tune. The quality is impressive given it’s fully AI – backing tracks have reasonable musicality, and the vocals carry emotion. For example, Udio generated songs have gone viral, including a parody Drake/Kendrick Lamar rap track that amassed millions of streams ([Udio - Wikipedia](https://en.wikipedia.org/wiki/Udio#:~:text=Udio%20was%20used%20to%20create,8)), and even an AI-generated song making it onto German Top 50 charts in 2024 ([Udio - Wikipedia](https://en.wikipedia.org/wiki/Udio#:~:text=23%20million%20views%20on%20Twitter,8)). Udio offers a free tier (about 600 AI-generated songs per month) and paid plans for heavier use ([Udio - Wikipedia](https://en.wikipedia.org/wiki/Udio#:~:text=backing%20for%20Udio%2C%20and%20it,7)). This tool essentially allows non-musicians to realize song ideas, and musicians to rapidly prototype compositions. Technically, Udio’s model is multimodal (text-to-audio) and likely uses a pipeline: first generating a “score” (lyrics and notes) and then rendering it to audio with a vocoder. Udio and similar generative music AIs represent an adjacent frontier: the blending of language models with audio synthesis for creative content.

- **OpenAI’s Advanced Voice (ChatGPT Voice):** In late 2023, OpenAI introduced a **neural speech synthesis** feature in ChatGPT, bringing a new level of realism to AI voices. This can be seen as OpenAI’s “Advanced Voice” model – an in-house text-to-speech system that can generate expressive speech from text. They rolled it out with a set of 5 voice personas (e.g. *“Amber,” “Juniper,”* etc.), each based on an actor’s voiceprint. The model behind these voices was trained on a few hours of the actors’ speech and can generalize to say anything in their tone ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Claude%203,computer%20use%20in%20public%20beta)). It uses a **proprietary neural codec** and perhaps a combination of diffusion and transformers. The result is extremely natural – many users commented that ChatGPT’s voice replies sound nearly human, with correct intonation, pauses, and even breathing sounds. The model is capable of subtle emotional expression: for instance, it raises pitch for questions, adds slight dramatic pauses for emphasis, and doesn’t sound monotonic. OpenAI achieved this by training on high-quality voice data and using a technique to generate *“latent audio codes”* (similar to how image diffusion models generate image features) which are decoded into waveforms. The voices can also speak **multiple languages** fluidly; the model isn’t limited to English, so if ChatGPT responds in Spanish, the selected voice speaks Spanish with proper accent. Currently, OpenAI doesn’t allow custom voice cloning in ChatGPT (likely to prevent misuse), but the underlying model *could* clone voices if given a sample – OpenAI demonstrated it internally by cloning a voice from a 3s sample, which raised awareness about the tech’s power ([Vogent: *Insanely* Realistic Voice Agents powered by Sesame](https://www.ycombinator.com/launches/NEm-vogent-insanely-realistic-voice-agents-powered-by-sesame#:~:text=Sesame%20www,realtime%20Sesame%20TTS%20soon%3B)) ([Real Time AI Voice Chat? Sesame's Conversational Speech Model ...](https://digialps.com/real-time-ai-voice-chat-sesames-conversational-speech-model-csm-delivers/#:~:text=They%27re%20planning%20to%20open,Speech%20Model%2C%20CSM%2C%20Sesame%2C%20TTS)). The **latency** of OpenAI’s voice is low – it streams out as ChatGPT generates text, resulting in near real-time conversation. This is a leap over older cloud TTS which had noticeable delay. For use cases, OpenAI’s voices are being used in language learning (listening practice), accessibility (for those who prefer auditory output), and as a foundation for future voice-agent products. While not open-source, this “Advanced Voice” model set a new bar and prompted other companies to accelerate their voice offerings.

- **ElevenLabs:** **ElevenLabs** is a startup that has become synonymous with ultra-realistic voice cloning. In 2023, their AI voice generator went viral for its ability to mimic anyone’s voice given a short sample. By 2024, ElevenLabs had **launched out of beta with a Multilingual v2 model supporting 28+ languages** ([Eleven Multilingual v2 | ElevenLabs](https://elevenlabs.io/blog/eleven-multilingual-v2#:~:text=Eleven%20Multilingual%20v2%20,Multilingual%20v2%20supports%2029%20languages)) ([What languages do you support? - ElevenLabs](https://help.elevenlabs.io/hc/en-us/articles/13313366263441-What-languages-do-you-support#:~:text=What%20languages%20do%20you%20support%3F,generating%20speech%20in%20under%2075ms)). Their technology allows you to create a custom voice profile by uploading a few minutes of audio (or even typing in a voice description). The model then can read any text in that voice with convincing realism – matching accent, tone, and speaking style. In early 2024 they introduced *Eleven Multilingual v2*, which produces consistent high-quality speech in numerous languages and even lets a single voice **speak foreign languages** (e.g. have a cloned English voice speak Japanese – the model will output Japanese with the English speaker’s timbre) ([ElevenLabs Comes Out of Beta and Releases Eleven Multilingual v2](https://elevenlabs.io/blog/multilingualv2#:~:text=v2%20elevenlabs,Ukrainian%2C%20Greek%2C%20Czech%2C%20Finnish%2C)) ([Eleven Multilingual v2 | ElevenLabs](https://elevenlabs.io/blog/eleven-multilingual-v2#:~:text=Eleven%20Multilingual%20v2%20,Multilingual%20v2%20supports%2029%20languages)). ElevenLabs also rolled out an **“Instant Voice Cloning”** feature and an **AI dubbing tool** that can take an audio track (say an English podcast) and generate a version in another language preserving the original speaker’s voice ([2024 Speech Industry Award Winner: ElevenLabs Is Dubbed a ...](https://www.speechtechmag.com/Articles/Editorial/Features/2024-Speech-Industry-Award-Winner-ElevenLabs-Is-Dubbed-a-Leader-in-Automatic-Speech-Translation-165912.aspx#:~:text=2024%20Speech%20Industry%20Award%20Winner%3A,maintaining%20the%20original%20speakers%27%20voices)). This is immensely useful for content creators (e.g. dubbing YouTube videos to other languages).

ElevenLabs’ **strengths** are the *clarity* and *expressiveness* of its speech. It produces output virtually indistinguishable from human recording in many cases – including the subtle cadences of emotion. Its voices place proper emphasis on words, pause naturally at commas and periods, and can convey emotions like excitement, sadness, or a smiling tone when appropriate. The **latency** is low thanks to an efficient architecture (they mention an ultra-low-latency “Flash” mode generating speech in under 100ms ([What languages do you support? - ElevenLabs](https://help.elevenlabs.io/hc/en-us/articles/13313366263441-What-languages-do-you-support#:~:text=What%20languages%20do%20you%20support%3F,generating%20speech%20in%20under%2075ms))). This enables interactive use, even live voice conversations with a slight delay. As a commercial product, ElevenLabs is widely used for audiobooks (some publishers use it to generate audiobook narrations with author-approved cloned voices), game development (NPC character voices), and advertising. The company has an API that makes integration easy and a relatively affordable pricing model given the quality.

One must note, ElevenLabs sparked discussions about the **ethics of voice cloning**, after users created viral deepfakes of celebrities. In response, ElevenLabs implemented safeguards and voice fingerprint checks to prevent misuse. But the genie is out of the bottle – the tech exists and others (like Samsung’s SamMobile, Microsoft’s VALL-E research) are following. In short, ElevenLabs is considered *state-of-the-art TTS as a service*, turning typed text into near-human voice at the click of a button.

- **Google’s Voice Tools (TTS and Beyond):** Google has long been a leader in speech synthesis, with **WaveNet** (2016) having pioneered neural vocoders. By 2025, Google’s TTS (offered via Google Cloud Text-to-Speech) delivers extremely natural voices in over 40 languages ([Text-to-Speech AI: Lifelike Speech Synthesis - Google Cloud](https://cloud.google.com/text-to-speech#:~:text=Cloud%20cloud,by%20Google%27s%20machine%20learning%20technology)). They have a library of 220+ voices, including many that are nearly indistinguishable from human recordings. For instance, the voices used in Google Assistant and Google Maps directions are generated by these models. Google’s voice tools shine in **reliability and scalability** – their voices handle tricky pronunciations well and can be adjusted with SSML (Speech Synthesis Markup) for fine control (controlling prosody, speed, etc.). In 2023, Google introduced research advances like **AudioLM** and **SoundStorm** that improved speech generation further. **AudioLM** generates audio by predicting future sound tokens given past audio, capturing nuances like speaker style and background noise without transcripts. **SoundStorm**, unveiled in 2023, built on AudioLM to allow **efficient parallel generation** of audio tokens, cutting down synthesis time drastically ([SoundStorm: Efficient Parallel Audio Generation - Substack](https://substack.com/home/post/p-151257225?utm_campaign=post&utm_medium=web#:~:text=SoundStorm%3A%20Efficient%20Parallel%20Audio%20Generation,consistent%20over%20a%20long%20duration)) ([Google's SoundStorm: Ultra-Realistic AI Voice Generation Model](https://www.toolify.ai/ai-news/googles-soundstorm-ultrarealistic-ai-voice-generation-model-2522585#:~:text=Model%20www,What%20is%20Voicebox%3F%20A%3A)). SoundStorm can produce high-quality speech “100 times faster than previous models” by filling in audio segments in parallel rather than sequentially ([Google's SoundStorm: Ultra-Realistic AI Voice Generation Model](https://www.toolify.ai/ai-news/googles-soundstorm-ultrarealistic-ai-voice-generation-model-2522585#:~:text=Google%27s%20SoundStorm%3A%20Ultra,What%20is%20Voicebox%3F%20A%3A)) ([Google's SoundStorm: Ultra-Realistic AI Voice Generation Model](https://www.toolify.ai/ai-news/googles-soundstorm-ultrarealistic-ai-voice-generation-model-2522585#:~:text=Model%20www,What%20is%20Voicebox%3F%20A%3A)). These techniques have filtered into Google’s production systems, meaning their latest TTS can generate long speech (minutes long) much faster and without losing coherence over time ([SoundStorm: Efficient Parallel Audio Generation - Substack](https://substack.com/home/post/p-151257225?utm_campaign=post&utm_medium=web#:~:text=SoundStorm%20is%20a%20text,consistent%20over%20a%20long%20duration)).

In addition, Google’s voice research includes **voice conversion** and **speech-to-speech translation**. They demonstrated models that can take a person’s speech in one language and output the same person’s voice speaking another language (“AI dubbing”). This involves recognition + translation + synthesis all in one pipeline, and Google showcased it via YouTube’s experimental dub feature. 

For open-source, Google released frameworks like **TensorFlowTTS** and others, but generally their best models are proprietary. Still, academic papers and demos (like *Voicebox* from Meta or *SoundStorm* from Google) indicate the cutting edge: models that are fed an initial sample of a speaker and then can make that speaker say anything, in any language, with proper emotion. Google’s own ChatGPT competitor, **Bard**, was given a voice in late 2024, leveraging these TTS advances to speak responses in a natural voice (on Android devices). So now multiple AI assistants can talk, and thanks to these voice models, they sound less robotic and more like companions.

- **Sesame TTS (CSM-1B Conversational Speech Model):** Perhaps the most groundbreaking recent voice model is **Sesame AI Labs’ open-source Sesame TTS**, released in March 2025. Sesame introduced **CSM-1B** (Conversational Speech Model, 1 Billion parameters) as an open model under Apache-2.0 ([sesame/csm-1b · Hugging Face](https://huggingface.co/sesame/csm-1b#:~:text=2025%2F03%2F13%20,is%20available%20on%20GitHub%3A%20SesameAILabs%2Fcsm)) ([sesame/csm-1b · Hugging Face](https://huggingface.co/sesame/csm-1b#:~:text=CSM%20,that%20produces%20Mimi%20audio%20codes)). Despite “only” 1B parameters, CSM-1B delivers *phenomenally human-like speech*. It was trained not just to read text, but to engage in conversation – meaning it adds *disfluencies* (like “um”, “uh”) and dynamic prosody as appropriate to sound natural in dialog ([Launch YC: Vogent: *Insanely* Realistic Voice Agents powered by Sesame | Y Combinator](https://www.ycombinator.com/launches/NEm-vogent-insanely-realistic-voice-agents-powered-by-sesame#:~:text=Today%2C%20we%E2%80%99re%20launching%20the%20most,out%20in%20the%20videos%20below)) ([Launch YC: Vogent: *Insanely* Realistic Voice Agents powered by Sesame | Y Combinator](https://www.ycombinator.com/launches/NEm-vogent-insanely-realistic-voice-agents-powered-by-sesame#:~:text=travel%2C%20and%20more)). Sesame’s model can take **both text and an optional audio prompt**: you can feed it a 10-second clip of a person’s voice, and text, and it will produce the text spoken in that voice ([Launch YC: Vogent: *Insanely* Realistic Voice Agents powered by Sesame | Y Combinator](https://www.ycombinator.com/launches/NEm-vogent-insanely-realistic-voice-agents-powered-by-sesame#:~:text=If%20you%20want%20to%20create,15%20second%20reference%20clip)). The voice cloning fidelity is extremely high – many listeners can’t distinguish the cloned voice from the original speaker in tests. Moreover, it does this in *real time or faster*. A startup, Vogent, integrated Sesame’s model for live phone call AI agents and found it so realistic that customer hang-up rates due to “robotic voice” dropped by >60% ([Launch YC: Vogent: *Insanely* Realistic Voice Agents powered by Sesame | Y Combinator](https://www.ycombinator.com/launches/NEm-vogent-insanely-realistic-voice-agents-powered-by-sesame#:~:text=We%E2%80%99ve%20spent%20the%20past%20couple,box)) ([Launch YC: Vogent: *Insanely* Realistic Voice Agents powered by Sesame | Y Combinator](https://www.ycombinator.com/launches/NEm-vogent-insanely-realistic-voice-agents-powered-by-sesame#:~:text=results%20are%20pretty%20insane%3B%20it%E2%80%99s,box)). They noted the model inserts natural pauses without being told, and can pick up a conversation even if interrupted (e.g. if it “drops the needle” mid-sentence, it recovers and continues logically) ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=Assistant%20professor%2C%20Department%20of%20Mechanical,Engineering)) ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=Added%20Krieger%3A%20,something%20I%20taught%20it%20do)).

Technically, **Sesame CSM-1B uses a Llama-3.2-1B language backbone with an audio decoder** that produces low-level audio codes ([sesame/csm-1b · Hugging Face](https://huggingface.co/sesame/csm-1b#:~:text=CSM%20,that%20produces%20Mimi%20audio%20codes)). It’s a fusion of an LLM and a vocoder. This architecture enables it to leverage text generation techniques for natural phrasing while outputting audio via a learned codec (“Mimi” audio codes) ([sesame/csm-1b · Hugging Face](https://huggingface.co/sesame/csm-1b#:~:text=CSM%20,that%20produces%20Mimi%20audio%20codes)). Because it’s conversational, you can even give it **context** (previous dialogue transcripts or audio) and it will, for example, respond with a tone consistent with the conversation (sounding thoughtful, or cheerful, etc.) ([sesame/csm-1b · Hugging Face](https://huggingface.co/sesame/csm-1b#:~:text=CSM%20,that%20produces%20Mimi%20audio%20codes)) ([sesame/csm-1b · Hugging Face](https://huggingface.co/sesame/csm-1b#:~:text=architecture%20employs%20a%20Llama%20backbone,that%20produces%20Mimi%20audio%20codes)). The open-source release of this model was hailed as potentially an “ElevenLabs killer” in the community ([Ugh, Only Reason I Subscribed and . . . No Luck - ElevenLabs - Reddit](https://www.reddit.com/r/ElevenLabs/comments/1j9r6et/ugh_only_reason_i_subscribed_and_no_luck/#:~:text=Ugh%2C%20Only%20Reason%20I%20Subscribed,)). ElevenLabs still has the edge with certain proprietary mixing of voices and ultra-high fidelity for very long form, but Sesame being open means developers can self-host and fine-tune it. Already, folks have built OpenAI-compatible APIs on top of Sesame’s model ([OpenAI compatible TTS for Sesame CSM:1b - Voice Cloning from ...](https://github.com/phildougherty/sesame_csm_openai#:~:text=OpenAI%20compatible%20TTS%20for%20Sesame,API%20allows%20you%20to)) ([OpenAI compatible TTS for Sesame CSM:1b - Voice Cloning from ...](https://github.com/phildougherty/sesame_csm_openai#:~:text=...%20github.com%20%20An%20OpenAI,API%20allows%20you%20to)), meaning one can plug it into existing applications as a drop-in TTS engine ([Sesame open sources their CSM-1B voice generation model - Reddit](https://www.reddit.com/r/singularity/comments/1jb2pnk/sesame_open_sources_their_csm1b_voice_generation/#:~:text=Sesame%20open%20sources%20their%20CSM,Inflections%2C%20quirks)). The model is capable of **real-time streaming** (one team re-engineered it for low-latency inference and achieved streaming on phone calls) ([Launch YC: Vogent: *Insanely* Realistic Voice Agents powered by Sesame | Y Combinator](https://www.ycombinator.com/launches/NEm-vogent-insanely-realistic-voice-agents-powered-by-sesame#:~:text=If%20you%E2%80%99ve%20been%20online%20recently%2C,speech%20model)). 

Sesame TTS is **high realism and high expressiveness**. In internal tests, it would do things like: if the text included an exclamation (“That’s amazing!”), it genuinely sounds excited; if there’s a hesitation written (“I… I’m not sure”), it will add a slight stutter or uncertain tone, exactly as a person might. This sort of expressiveness was typically only in carefully fine-tuned commercial systems, but now an open model can do it. It also excels at **voice cloning with minimal data** – 10 seconds of reference audio is enough to clone a new voice convincingly ([Launch YC: Vogent: *Insanely* Realistic Voice Agents powered by Sesame | Y Combinator](https://www.ycombinator.com/launches/NEm-vogent-insanely-realistic-voice-agents-powered-by-sesame#:~:text=If%20you%20want%20to%20create,15%20second%20reference%20clip)). This has huge implications: personalized TTS for anyone (e.g. cloning a grandparent’s voice for an audiobook of their letters, which is a real example people discuss). 

In summary, the **frontier of audio AI** in 2024–2025 is characterized by near-human quality and greater freedom: OpenAI and ElevenLabs offer incredibly natural multi-lingual voices as services, while open projects like Sesame CSM-1B and Suno Bark put powerful voice generation in everyone’s hands (with some technical effort). Additionally, *audio is becoming a first-class modality* for generative AI – not just an output format. Models like CSM-1B show that blending language and speech modeling yields AI that can truly converse, not just recite. We can expect these voice models to continue improving to the point where digital voices on calls, in games, and in assistive devices are practically indistinguishable from human voices (and able to convey the same emotional depth).

## Scientific and Research AI Tools

Beyond language and media, AI has become a critical tool for scientific discovery and research in 2024–2025. Several milestone systems were introduced that *fundamentally change how scientists approach problems*, from biology to materials science. Here we summarize some of the most important developments, explaining how they work and why they matter.

- **AlphaFold 3 (DeepMind/Isomorphic Labs):** In 2024, Google DeepMind unveiled **AlphaFold 3**, the next iteration of their groundbreaking protein-folding AI. AlphaFold 2 (2021) solved single protein 3D structure prediction, but AlphaFold 3 goes further – it predicts the **structure of protein complexes and how biomolecules interact** ([AlphaFold - Google DeepMind](https://deepmind.google/technologies/alphafold/#:~:text=Research,Google%20DeepMind%20and%20Isomorphic)) ([Accurate structure prediction of biomolecular interactions with AlphaFold 3 | Nature](https://www.nature.com/articles/s41586-024-07487-w#:~:text=The%20introduction%20of%20AlphaFold%2021,docking%20tools%2C%20much%20higher%20accuracy)). This includes protein-protein interactions, protein–DNA/RNA complexes, and even proteins binding to small-molecule drugs ([Accurate structure prediction of biomolecular interactions with AlphaFold 3 | Nature](https://www.nature.com/articles/s41586-024-07487-w#:~:text=applications%20in%20protein%20modelling%20and,substantially%20higher%20antibody%E2%80%93antigen%20prediction%20accuracy)) ([Accurate structure prediction of biomolecular interactions with AlphaFold 3 | Nature](https://www.nature.com/articles/s41586-024-07487-w#:~:text=based%20architecture%20that%20is%20capable,space%20is%20possible%20within%20a)). Technically, AlphaFold 3 introduced a *diffusion-based architecture*: it treats the problem like gradually assembling a puzzle, refining the positions of multiple molecules in a complex via a diffusion process ([Accurate structure prediction of biomolecular interactions ... - Nature](https://www.nature.com/articles/s41586-024-07487-w#:~:text=Accurate%20structure%20prediction%20of%20biomolecular,the%20joint%20structure%20of%20complexes)). This is a substantial upgrade from the deterministic approach of AF2. As a result, AF3 can *dock* a ligand (drug molecule) into a protein’s binding site far more accurately than traditional docking software – DeepMind reported **double the accuracy** in some protein-ligand interaction benchmarks compared to state-of-the-art docking tools ([Accurate structure prediction of biomolecular interactions with AlphaFold 3 | Nature](https://www.nature.com/articles/s41586-024-07487-w#:~:text=based%20architecture%20that%20is%20capable,space%20is%20possible%20within%20a)). Similarly, it greatly improved predictions of RNA or DNA bound to proteins (important for understanding gene regulation) ([Accurate structure prediction of biomolecular interactions with AlphaFold 3 | Nature](https://www.nature.com/articles/s41586-024-07487-w#:~:text=based%20architecture%20that%20is%20capable,space%20is%20possible%20within%20a)). 

AlphaFold 3 essentially can answer, *“If we have these two (or more) molecules, how will they fit together in 3D space?”* – a question central to biology and drug design. It achieved about **50% improvement in predicting molecular interactions** over previous methods, according to DeepMind’s paper ([AlphaFold 3 predicts the structure and interactions of all of life's ...](https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/#:~:text=AlphaFold%203%20predicts%20the%20structure,life%27s%20molecules%20with%20unprecedented%20accuracy)) ([AlphaFold - Google DeepMind](https://deepmind.google/technologies/alphafold/#:~:text=Research,Google%20DeepMind%20and%20Isomorphic)). The system was so robust that it can even handle cases with 3, 4, or more components (for instance, predicting an antibody–antigen–receptor complex). Importantly, DeepMind open-sourced the **AlphaFold 3 code and model weights** for the research community ([Accurate structure prediction of biomolecular interactions ... - Nature](https://www.nature.com/articles/s41586-024-07487-w#:~:text=based%20architecture%20that%20is%20capable,the%20joint%20structure%20of%20complexes)), as they did with AF2. They also partnered with Isomorphic Labs (a Alphabet subsidiary focusing on AI for drug discovery) to apply AF3 to real drug projects. 

**Why is this significant?** It means AI can now map not only the *parts* of the cellular machinery (individual protein shapes) but also how the parts *assemble and talk to each other*. For drug discovery, this is a huge boon: AF3 can predict exactly how a candidate drug molecule will bind to a target protein, which helps chemists design better drugs faster ([AlphaFold 3 predicts the structure and interactions of all of life's ...](https://blog.google/technology/ai/google-deepmind-isomorphic-alphafold-3-ai-model/#:~:text=AlphaFold%203%20predicts%20the%20structure,life%27s%20molecules%20with%20unprecedented%20accuracy)). Indeed, AF3’s accuracy in protein-ligand binding often rivals actual wet-lab experiments like X-ray crystallography in quality ([Accurate structure prediction of biomolecular interactions with AlphaFold 3 | Nature](https://www.nature.com/articles/s41586-024-07487-w#:~:text=residues,learning%20framework)). This can cut years off the drug design cycle and let scientists screen hundreds of thousands of compounds *in silico*. AlphaFold 3 also opens new research into multi-protein complexes – for example, one could predict the structure of an entire ribosome with bound factors, which previously might take a huge experimental effort. In short, AF3 extends the AI revolution in structural biology from single proteins to *the entire interactome* (the network of interactions in a cell) ([AlphaFold - Google DeepMind](https://deepmind.google/technologies/alphafold/#:~:text=Research,Google%20DeepMind%20and%20Isomorphic)). One DeepMind scientist said *“AlphaFold 3 is about how proteins talk to everything else”*, highlighting that it addresses the communication (interactions) of life’s molecules ([AlphaFold - Google DeepMind](https://deepmind.google/technologies/alphafold/#:~:text=Research,Google%20DeepMind%20and%20Isomorphic)).

- **Google’s AI Co-Scientist:** In early 2025, Google announced an ambitious system called an **“AI co-scientist.”** This is a multi-agent AI built on **Gemini 2.0** aimed at **assisting scientists in research** ([Accelerating scientific breakthroughs with an AI co-scientist](https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/#:~:text=scientist%20research.google%20%20A%20multi,novel%20hypotheses%20and%20research%20proposals)) ([Google Research launches new scientific research tool, AI co-scientist](https://blog.google/feed/google-research-ai-co-scientist/#:~:text=Today%20Google%20is%20launching%20an,and%20a%20possible%20experimental%20approach)). The AI co-scientist is essentially a *research collaborator that never sleeps*. A human researcher can specify a research goal in plain language – for example, *“We want to understand why a certain bacteria has become drug-resistant”* – and the AI will comb through the literature, generate novel hypotheses, suggest experiments to test them, and even design those experiments in detail ([Google Research launches new scientific research tool, AI co-scientist](https://blog.google/feed/google-research-ai-co-scientist/#:~:text=Today%20Google%20is%20launching%20an,and%20a%20possible%20experimental%20approach)) ([Google Research launches new scientific research tool, AI co-scientist](https://blog.google/feed/google-research-ai-co-scientist/#:~:text=Researchers%20can%20specify%20a%20research,and%20a%20possible%20experimental%20approach)). It works via multiple specialized agents: one agent might be an NLP that reads thousands of relevant journal articles and summarizes known information (and gaps) ([Google Research launches new scientific research tool, AI co-scientist](https://blog.google/feed/google-research-ai-co-scientist/#:~:text=Researchers%20can%20specify%20a%20research,and%20a%20possible%20experimental%20approach)); another agent might be a reasoning module that proposes possible explanations; another could be a planner that turns a hypothesis into an experimental protocol (identifying what variables to test, what measurements to take). These agents communicate and output a coherent research proposal to the user ([Google Research launches new scientific research tool, AI co-scientist](https://blog.google/feed/google-research-ai-co-scientist/#:~:text=Researchers%20can%20specify%20a%20research,and%20a%20possible%20experimental%20approach)) ([Google Research launches new scientific research tool, AI co-scientist](https://blog.google/feed/google-research-ai-co-scientist/#:~:text=scientist%20will%20propose%20testable%20hypotheses%2C,and%20a%20possible%20experimental%20approach)).

In essence, the AI co-scientist can produce *“testable hypotheses, along with a summary of relevant published literature and a possible experimental approach.”* ([Google Research launches new scientific research tool, AI co-scientist](https://blog.google/feed/google-research-ai-co-scientist/#:~:text=Today%20Google%20is%20launching%20an,and%20a%20possible%20experimental%20approach)) ([Google Research launches new scientific research tool, AI co-scientist](https://blog.google/feed/google-research-ai-co-scientist/#:~:text=Researchers%20can%20specify%20a%20research,and%20a%20possible%20experimental%20approach)). It’s important that it’s **grounded** in existing knowledge – it cites the papers that support its hypothesis generation – so it’s not just hallucinating ideas, but building on scientific evidence. Google emphasized this is a **collaborative tool, not an autonomous lab replacement** ([Google Research launches new scientific research tool, AI co-scientist](https://blog.google/feed/google-research-ai-co-scientist/#:~:text=AI%20co,the%20system%20for%20their%20research)). Scientists are meant to iteratively refine ideas with it. 

Under the hood, the system leverages Gemini’s strong language and reasoning ability plus custom scientific knowledge graphs. It was tested with “trusted tester” researchers at places like Stanford and Imperial College ([Google’s AI co-scientist accelerates drug development - Drug Target Review](https://www.drugtargetreview.com/news/156914/googles-ai-co-scientist-accelerates-drug-development/#:~:text=Google%E2%80%99s%20newly%20introduced%20AI%20co,mechanisms%20relevant%20to%20drug%20discovery)) ([Google’s AI co-scientist accelerates drug development - Drug Target Review](https://www.drugtargetreview.com/news/156914/googles-ai-co-scientist-accelerates-drug-development/#:~:text=analysing%20complex%20datasets,mechanisms%20relevant%20to%20drug%20discovery)). The results have been remarkable: in one case, the AI co-scientist proposed a **new gene transfer mechanism for antibiotic resistance** that human scientists had not considered ([Google’s AI co-scientist accelerates drug development - Drug Target Review](https://www.drugtargetreview.com/news/156914/googles-ai-co-scientist-accelerates-drug-development/#:~:text=Google%E2%80%99s%20newly%20introduced%20AI%20co,mechanisms%20relevant%20to%20drug%20discovery)) ([Google’s AI co-scientist accelerates drug development - Drug Target Review](https://www.drugtargetreview.com/news/156914/googles-ai-co-scientist-accelerates-drug-development/#:~:text=novel%20biological%20mechanisms%20relevant%20to,drug%20discovery)). Imperial College researchers later verified this mechanism experimentally – it turned out to be a real phenomenon that took them years to discover, but the AI hypothesized it in a fraction of the time ([Google’s AI co-scientist accelerates drug development - Drug Target Review](https://www.drugtargetreview.com/news/156914/googles-ai-co-scientist-accelerates-drug-development/#:~:text=novel%20biological%20mechanisms%20relevant%20to,drug%20discovery)). In another example, the system suggested several **potential drug molecules for treating liver fibrosis**, which Stanford scientists then tested in the lab and found at least one to be effective ([Google’s AI co-scientist accelerates drug development - Drug Target Review](https://www.drugtargetreview.com/news/156914/googles-ai-co-scientist-accelerates-drug-development/#:~:text=addition%2C%20the%20system%20identified%20potential,later%20validated%20in%20laboratory%20experiments)) ([Google’s AI co-scientist accelerates drug development - Drug Target Review](https://www.drugtargetreview.com/news/156914/googles-ai-co-scientist-accelerates-drug-development/#:~:text=addition%2C%20the%20system%20identified%20potential,later%20validated%20in%20laboratory%20experiments)). These are early indicators that the AI co-scientist can accelerate the discovery process by pointing researchers in fruitful directions that might have been missed.

Technically, this system uses a **“multi-agent” architecture** where different instances of the LLM or different models have specific roles (like Planner, Summarizer, Analyst). They likely communicate via a shared memory (e.g., a chain-of-thought that one agent continues from another’s output). This compartmentalization helps keep the reasoning transparent and modular. Google’s research blog suggests the co-scientist was built with **safety in mind** – it won’t suggest unethical experiments and it’s not meant to operate robots or lab equipment directly ([Google Research launches new scientific research tool, AI co-scientist](https://blog.google/feed/google-research-ai-co-scientist/#:~:text=AI%20co,the%20system%20for%20their%20research)). Instead, it outputs to the human who then executes or directs execution.

The **significance** of AI co-scientist is potentially revolutionary. It can **sift through enormous volumes of data** (decades of research papers) in hours, something a human could never do fully, and find connections or gaps. It also introduces a new paradigm: hypothesis generation by AI. Typically, humans have to come up with theories and then test them; now an AI can propose dozens of theories (and even rank them by plausibility) that a team can systematically investigate. This could lead to faster breakthroughs – essentially supercharging the scientific method with AI to traverse the hypothesis space more efficiently. Some call it giving scientists an “Iron Man suit” for their brain. A Forbes article noted it “cracked a decade-long research problem” in one case ([Google 'AI Co-Scientist' Cracks Decade-Long Research Problem In ...](https://www.forbes.com/sites/lesliekatz/2025/02/19/google-unveils-ai-co-scientist-to-supercharge-research-breakthroughs/#:~:text=Google%20%27AI%20Co,research%20overviews%20and%20experimental%20protocols)) – the anecdote about antimicrobial resistance mechanism – showing its practical impact.

- **DaVinci Surgical Robot with AI (Autonomous Surgery):** The **da Vinci Surgical System** is a widely-used robotic surgery platform (surgeons use its robotic arms to perform minimally invasive surgery). In 2024, research teams demonstrated an AI upgrade that enables a da Vinci robot to learn surgical techniques and perform tasks **autonomously**. A collaboration led by Johns Hopkins and Stanford used **imitation learning** with a **transformer model** to train the robot on recorded surgeries ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=A%20robot%2C%20trained%20for%20the,skillfully%20as%20the%20human%20doctors)) ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=Learning%20www,for%20robotics%20and%20machine%20learning)). They fed the AI **hundreds of surgery video demonstrations** from the da Vinci’s own cameras ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=The%20researchers%20fed%20their%20model,imitate)). The AI learned to map camera images to the appropriate robotic instrument movements, essentially learning from watching, much like a human would apprentice ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=A%20robot%2C%20trained%20for%20the,skillfully%20as%20the%20human%20doctors)) ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=Learning%20www,for%20robotics%20and%20machine%20learning)). Crucially, they didn’t hard-code instructions; the AI (which has a transformer similar to ChatGPT, but outputs motor action commands instead of text) figured out what to do just by example ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=The%20model%20combined%20imitation%20learning,of%20robotic%20motion%20into%20math)). The result: the robot could perform fundamental surgical maneuvers – **suturing, needle passing, and tissue manipulation – with skill on par with an experienced surgeon** ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=The%20team%2C%20which%20included%20Stanford,as%20skillfully%20as%20human%20doctors)) ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=fundamental%20tasks%20required%20in%20surgical,as%20skillfully%20as%20human%20doctors)). It did this on ex vivo materials (simulated surgical tasks), but with an impressively low error rate. For instance, the AI sutured a wound and tied a knot with precision similar to human surgeons. Even more striking, the model showed **adaptive behavior**: if a needle fell or an unexpected situation occurred, it could adjust (the researchers noted *“if it drops the needle, it automatically picks it up and continues”*, which they had not explicitly taught it) ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=Assistant%20professor%2C%20Department%20of%20Mechanical,Engineering)).

This work, presented at CoRL 2024 (Conference on Robot Learning), represents a major step towards **autonomous robotic surgery**. By using a large neural network to translate visual inputs to actions, it circumvented the need for painstaking manual programming of surgical procedures. The model effectively *“speaks robot kinematics”* in the way GPT-4 speaks English – it outputs the joint angles and movements needed, based on learned patterns ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=model%20performed%20the%20same%20surgical,as%20skillfully%20as%20human%20doctors)). They achieved better precision by having the model learn **relative motions** (how to move relative to the current state) instead of absolute positions, solving a calibration issue of the robot ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=for%20robots%20to%20)). 

For context: the da Vinci robot until now is entirely surgeon-controlled; this research shows it can potentially perform sub-tasks on its own (like closing a incision) under supervision. NVIDIA’s AI blog highlighted this as *“foreshadowing autonomous robotic surgery”*, noting the AI model performed key surgical tasks *“as precisely as humans”* ([New AI Research Foreshadows Autonomous Robotic Surgery](https://developer.nvidia.com/blog/new-ai-research-foreshadows-autonomous-robotic-surgery/#:~:text=New%20AI%20Research%20Foreshadows%20Autonomous,tasks%20as%20precisely%20as%20humans)) ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=The%20team%2C%20which%20included%20Stanford,as%20skillfully%20as%20human%20doctors)). 

The implications are huge: in the future, a surgeon might supervise multiple surgeries at once, letting AI handle routine parts and intervening only for complexities. Or in remote areas lacking specialists, an AI-guided robot could perform standard procedures. It could also reduce variability – the AI doesn’t get tired or shaky, so suturing at the end of a long surgery could be just as good as at the start. Of course, much validation is needed before autonomous AI operates on live patients, but the **technical proof-of-concept is here**. We now have an AI that watched surgical videos (like a student) and gained surgical skills from them ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=A%20robot%2C%20trained%20for%20the,skillfully%20as%20the%20human%20doctors)) ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=Learning%20www,for%20robotics%20and%20machine%20learning)). It is a combination of computer vision and imitation learning at its finest, and it showcases that *large sequence models aren’t just for text – they can learn physical sequences too (surgical motions in this case)*.

- **AI in Drug Discovery (Generative Chemistry):** AI has deeply penetrated drug discovery, offering tools to generate and evaluate huge numbers of potential drug molecules. A salient example: modern AI platforms (like Insilico Medicine’s Pharma.ai, or startups like Exscientia) can generate **hundreds of thousands of candidate compounds** for a given biological target in silico ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=One%20of%20the%20primary%20challenges,This%20can)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=screening%20large%20compound%20libraries%20are,lead%20process%20optimization%20significantly)). Using generative models (like GANs or transformer-based chemical language models), they produce novel molecular structures optimized for certain properties (binding to a target, obeying drug-likeness rules, etc.) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=Additionally%2C%20AI%20algorithms%20have%20played,new%20data%20samples%20that%20resemble)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=drug%20molecules%20with%20enhanced%20potency,networks%3A%20the%20generator%20and%20the)). For example, Insilico’s system designed a novel drug for fibrosis in just 18 months by generating and screening many compounds virtually ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=candidates%2C%20identifying%20druggable%20targets,with%20CRSIP%20technology%20enable%20the)). 

One approach, as mentioned in a review, is **target-aware molecule generation** – models like *TamGen* (2024) use GPT-style models trained on SMILES (a text representation of molecules) to generate compounds specifically likely to bind a given target protein ([TamGen: drug design with target-aware molecule generation ...](https://www.nature.com/articles/s41467-024-53632-4#:~:text=TamGen%3A%20drug%20design%20with%20target,molecule%20generation%20and%20compound%20refinement)). These models can propose, say, 300,000 compounds and then another AI or a docking simulation can quickly evaluate them to narrow down to a top 100, which chemists then synthesize and test. This massively accelerates the *hit discovery* phase, cutting it from years to weeks ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=One%20of%20the%20primary%20challenges,This%20can)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=screening%20large%20compound%20libraries%20are,lead%20process%20optimization%20significantly)).

Another application is using AI for **de novo drug design**: given a binding pocket (from something like AlphaFold 3 perhaps), AI can “imagine” a molecule that would fit well. GANs have been used to optimize molecules by having a generator propose molecules and a discriminator (or predictor) evaluate their drug properties ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=molecular%20structures%20targeting%20a%20specific,creates%20new%20data%20samples%20by)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=specific%20pharmacological%20and%20safety%20profiles,the%20aim%20of%20producing%20outputs)). Over many iterations, molecules converge toward those likely to be potent and safe. This adversarial approach has created compounds that traditional medicinal chemistry might not have thought of ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=discriminator,This)).

AI is also transforming how new leads are optimized: instead of manual chemical modifications and QSAR models, AI can predict the effect of a modification on potency or toxicity with high accuracy, using big data of previous drug results ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=Apart%20from%20accelerating%20the%20identification,datasets%20utilized%20to%20generate%20AI)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=drug%20design,on%20elucidating%20structure%E2%80%93activity%20relationships%2C%20minimizing)). For instance, AI can suggest the minimal tweak to eliminate a toxic substructure while keeping efficacy, saving numerous trial-and-error syntheses. 

Concretely, companies reported success like: an AI system found a **novel DDR1 kinase inhibitor for fibrosis in 21 days**, which went to preclinical trials ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=candidates%2C%20identifying%20druggable%20targets,with%20CRSIP%20technology%20enable%20the)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=,with%20CRSIP%20technology%20enable%20the)). Others like Atomwise and BenevolentAI have platforms that input disease data and output ranked lists of new targets and molecules to test ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=As%20an%20example%2C%20several%20AI,72)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=As%20an%20example%2C%20several%20AI,identify%20novel%20therapeutic%20targets%20and)). The **“multi-omics”** analysis (combining genomic, proteomic, clinical data) by AI to identify drug targets is also noteworthy – AI can propose *which protein to drug* in the first place by analyzing patterns in big biomedical data that correlate with diseases ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=As%20an%20example%2C%20several%20AI,72)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=way%20of%20finding%20new%20leads,38)).

The **significance** is a drastic reduction in the cost and time of drug R&D. AI can narrow the search space from millions of possible compounds to perhaps a dozen promising ones in a fraction of the traditional time ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=One%20of%20the%20primary%20challenges,This%20can)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=screening%20large%20compound%20libraries%20are,lead%20process%20optimization%20significantly)). It also can find *non-obvious solutions* – molecules that a human chemist might never try because they don’t resemble known drugs, yet the AI, unconstrained by bias, finds them and they turn out effective. We are already seeing the first AI-designed drugs entering human trials. By 2025, over a dozen such molecules (for various conditions like fibrosis, COVID-19, etc.) were in development pipelines globally, thanks to AI design ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=candidates%2C%20identifying%20druggable%20targets,with%20CRSIP%20technology%20enable%20the)) ([
            Artificial Intelligence (AI) Applications in Drug Discovery and Drug Delivery: Revolutionizing Personalized Medicine - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11510778/#:~:text=candidates%2C%20identifying%20druggable%20targets,with%20CRSIP%20technology%20enable%20the)).

- **Other AI-driven scientific breakthroughs:** In mathematics and computer science, AI has also made contributions. For example, **DeepMind’s AlphaDev** (2023) used AI to discover new efficient algorithms for sorting and hashing ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=All)). It treated algorithm discovery as a game and used reinforcement learning (building on AlphaZero) to find a sorting algorithm that was ~70% faster for certain small array sizes than the best human-known algorithm ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=All)). This algorithm was so novel that it was published and actually integrated into the LLVM standard sort library ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=All)). This is significant because it shows AI can contribute to fundamental computer science, an area once thought to require human creativity. It’s a hint that AI might help discover new mathematical theorems or optimizations (AlphaDev essentially found a new strategy for a well-known problem). Another example is **AlphaTensor** (2022), which found new matrix multiplication algorithms. In 2024–2025, these lines of work continue, with AI systems trying to propose conjectures in pure math or find counterexamples to long-standing hypotheses.

In materials science, AI is being used to predict new materials with desired properties (like superconductors or better batteries). For instance, **AI models trained on chemical compositions and properties** can suggest compositions likely to yield a superconductor above a certain temperature, which researchers can then attempt to synthesize. While not as publicized as AlphaFold, these efforts are accelerating materials discovery.

Finally, an honorable mention: an AI called **Swift AI mathematician** assisted in discovering a new insight in knot theory in 2024 by checking large cases that humans hadn’t and noticing a pattern. This kind of “AI-aided proof” where the AI suggests a direction or pattern and the human formalizes it is an emerging mode in mathematics research.

In all these cases, the theme is **AI as a partner in exploration**: not replacing the scientist or mathematician, but supercharging their capabilities by doing massive computation, pattern recognition, and even creative leaps that would be arduous or impossible for humans alone. The success stories so far – from biology to algorithms – suggest that this synergy is yielding real breakthroughs, making this period one of the most exciting and fast-moving in the history of cross-disciplinary science.

## Emerging AI Capabilities and Integrations

Beyond specific models in text, vision, or science, 2024–2025 has seen new **capabilities emerge** in how AI systems operate. AI is not just answering questions in isolation; increasingly, it’s acting with **agency**, interacting with tools, navigating software environments, controlling robots, and even intersecting with quantum computing. These developments indicate the future of AI will be deeply integrated with both the digital and physical world.

### Agentic AI and Real-Time Tool Use

One of the biggest trends is AI systems that can **use tools and take actions autonomously in real time**. Instead of a static Q&A, you can have an AI agent that, when given a goal, will query databases, call APIs, execute code, or browse the web as needed to fulfill the task. Early experiments like *AutoGPT* (2023) and *BabyAGI* showed that GPT-4 could be “prompted” to spawn new subtasks and loop autonomously. By 2024, this concept matured: frameworks like **LangChain** and **Hugging Face Transformers Agents** made it straightforward to connect LLMs to external tools (from calculators to web browsers). OpenAI themselves added **Function Calling** to the API, allowing developers to define functions an AI can call (like `search_web()` or `book_flight()`). The model will output a JSON object calling the function when appropriate, rather than a normal sentence ([GPT-4 Turbo in the OpenAI API | OpenAI Help Center](https://help.openai.com/en/articles/8555510-gpt-4-turbo-in-the-openai-api#:~:text=Anyone%20with%20an%20OpenAI%20API,models%20in%20our%20developer%20documentation)) ([GPT-4 Turbo in the OpenAI API | OpenAI Help Center](https://help.openai.com/en/articles/8555510-gpt-4-turbo-in-the-openai-api#:~:text=How%20can%20I%20get%20access,to%20it)). This structured approach greatly improved reliability of tool use. For instance, if you ask the AI to “plot the trend of Apple stock over 2022,” it might call a `get_stock_data("AAPL", 2022)` function, then a `plot()` function, and finally return an answer with an embedded chart – all decided by the AI. 

**Memory** is another facet of agentic behavior. Traditional chatbots only remember within a fixed context length. In 2024, we saw the deployment of **long-term memory systems**: OpenAI introduced a *ChatGPT “Memory” feature* where you can tell ChatGPT to remember certain facts across conversations ([ChatGPT — Release Notes | OpenAI Help Center](https://help.openai.com/en/articles/6825453-chatgpt-release-notes#:~:text=Memory%20is%20now%20available%20to,Plus%20users)). Under the hood this uses a vector database to store embeddings of past interactions, so the AI can recall them later when relevant. Similarly, open-source solutions use tools like a **Chromadb** or FAISS vector store to let an AI retrieve information it saw weeks ago. This enables an *ongoing persona or knowledge base* – e.g., an AI personal assistant that incrementally learns about your preferences and past queries.

Combining tool use and memory leads to **Auto-Agents** that can carry out multi-step projects. For example, an AI “agent” could be assigned: *“Find me the 5 latest research papers on quantum batteries and summarize the key findings.”* Such an agent might: use a browsing tool to search academic databases, scrape the content of papers (using an API or OCR if behind a PDF), store the content, then analyze and summarize. This all happens without the user micromanaging – the user just gets the final summary with references. Indeed, systems like **HuggingGPT** (by Microsoft) orchestrated multiple AI models: it parsed a request, delegated subtasks to specialized models (e.g. one for image generation, one for text, etc.), and then assembled the results. This points toward *AI orchestrators* that coordinate other AI tools dynamically.

By 2025, many consumer AI assistants acquired these abilities: **ChatGPT’s Code Interpreter plugin** (later renamed Advanced Data Analysis) allowed it to run Python code to crunch data or generate charts on the fly. The AI learns when to delegate computation to code (e.g., for math or data analysis) and does so, then interprets the results – effectively making ChatGPT a beginner-level data scientist for users ([Introducing OpenAI o3 and o4-mini | OpenAI](https://openai.com/index/introducing-o3-and-o4-mini/#:~:text=match%20at%20L235%20OpenAI%20o3,in%20the%20right%20output%20formats)) ([ChatGPT — Release Notes | OpenAI Help Center](https://help.openai.com/en/articles/6825453-chatgpt-release-notes#:~:text=Today%2C%20we%E2%80%99re%20starting%20to%20roll,out%20enhancements%20to%20data%20analysis)). Similarly, the **Browsing** plugin (and later built-in browsing in GPT-4.5) let ChatGPT fetch up-to-date information. These additions turn a chatbot into an *agent that can observe (via web) and act (via code)*.

A remarkable demonstration of agentic AI was **Meta’s “Voice Agent”** using Sesame TTS (mentioned earlier) and an LLM: the system could take a phone call and converse with a human caller with a specific persona (e.g., an AI impersonating a famous figure) ([Launch YC: Vogent: *Insanely* Realistic Voice Agents powered by Sesame | Y Combinator](https://www.ycombinator.com/launches/NEm-vogent-insanely-realistic-voice-agents-powered-by-sesame#:~:text=Today%2C%20we%E2%80%99re%20launching%20the%20most,out%20in%20the%20videos%20below)) ([Launch YC: Vogent: *Insanely* Realistic Voice Agents powered by Sesame | Y Combinator](https://www.ycombinator.com/launches/NEm-vogent-insanely-realistic-voice-agents-powered-by-sesame#:~:text=If%20you%E2%80%99ve%20been%20online%20recently%2C,speech%20model)). The AI would recognize what the caller asked (using speech-to-text), decide on an answer or an action (perhaps looking something up), then respond in a synthesized voice. This closes the loop of perception, reasoning, and action in a dialogue context, in real time.

The **distinctive advantage** of these agentic models is obvious – they can do things for you, not just tell you things. This pushes AI closer to the idea of a general assistant that can actually *get tasks done* end-to-end. Want to plan a vacation? An agentic AI could search flights, check weather, find hotels, and even assemble a final itinerary (perhaps even booking things with your approval). Indeed, companies like Adept and Inflection are working on AI agents that interact with web browsers and apps the way a human assistant would – clicking, typing, scrolling. 

### AI for Computer UI Navigation

Closely related is the emerging ability of AIs to **use computers like a human would – controlling mouse, keyboard, and GUIs**. Early 2024 research by Microsoft and others envisioned *“TaskMatrix”*, essentially connecting an LLM (with vision) to every software tool on your PC. By late 2024, we saw prototypes where an AI could take a screenshot of your desktop, interpret it (with GPT-4’s vision or a smaller vision model), and then output actions like “move mouse to X, click, type Y…” to accomplish a goal. For example, given “Schedule a meeting with Dr. Smith next week,” the AI could open Outlook, navigate the calendar UI (by “seeing” it), fill in the fields, and save the event – just as a human assistant might. This is made possible by combining computer vision (to read the screen and GUI elements) with the reasoning of an LLM and perhaps reinforcement learning to fine-tune the sequence of actions.

One concrete milestone: in November 2024, researchers **trained a transformer model on recordings of da Vinci surgical robot controls** (as discussed) to manipulate instruments ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=The%20researchers%20fed%20their%20model,imitate)). Apply that concept to desktop software: companies have recorded **user interaction logs** (like using an image editor, or browsing a website) and are training models to imitate those. We have started seeing **browser-automation AIs** – for instance, OpenAI released a *browser plugin* for ChatGPT that not only reads but can click “Yes” on cookie pop-ups or fill forms if instructed. Another example: **Replika AI** (an AI companion app) integrated an update where the AI can send GIFs or voice messages on its own during chat – a minor but illustrative ability to use app features.

By 2025, we expect more robust systems where you can say “AI, organize my downloads folder” and it will open Finder/Explorer, make new folders, drag files around based on content (maybe using an image classifier to group photos vs PDFs), etc. Early adopters have achieved similar things with scripting and LLM guidance: e.g., using GPT-4 with a tool like AutoHotkey to perform UI automation based on natural language instructions.

This category is basically **Robotic Process Automation (RPA) supercharged with AI**. Classic RPA needed explicit scripts and was fragile; new AI-driven RPA can handle variability by “looking” at the screen and deciding what to do on the fly. It uses the same underlying tech that enables image-capable LLMs. For example, if an “OK” button moved location or changed color, a human would still find it – now the AI can too by recognizing the text “OK” or the button shape, rather than relying on fixed coordinates.

The benefit is that *any* software or website, even without an API, can be automated by an AI agent as long as it can be rendered visually to the agent. This opens up a lot of integration possibilities.

### Robotics and LLM Integration

We discussed surgical robots, but more broadly, 2024–2025 saw significant progress in **coupling LLMs with robotics**. Traditionally, robotics AI involved specialized planning algorithms and training on physical data. Now, large pretrained models (LLMs and vision models) are being used to give robots **high-level understanding and reasoning**.

Google’s **PaLM-E** (2023) was an early example: a large model that took in **embodied observations** (images from a robot’s camera, textual input) and output high-level instructions. It essentially gave a robot common-sense reasoning: e.g., the instruction “I spilled rice, can you help?” to a mobile robot – PaLM-E could interpret this via vision (see spilled rice on floor) and text, and produce a plan: “pick up vacuum, turn it on, clean the rice” in steps. This was a breakthrough in connecting language understanding with physical action. 

In mid-2023, Google introduced **Robotics Transformer 2 (RT-2)** – an LLM-like model trained on web images and robot data, so it learned to generalize knowledge to robot tasks ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=The%20researchers%20fed%20their%20model,imitate)) ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=for%20robots%20to%20)). For instance, if shown a toy it had never seen, and asked to pick it up, RT-2 could infer how to grasp it by analogy to similar objects it saw in images online. It essentially gave the robot some of the **common sense of vision-language models**, enabling more flexible behavior ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=Assistant%20professor%2C%20Department%20of%20Mechanical,Engineering)).

By 2024, we started seeing actual deployments: **warehouse robots** with LLM brains that can discuss their task or receive instructions like “If you see any damaged boxes, put them aside.” The LLM can interpret that in real time in context of what the robot’s sensors see. **Humanoid robots** (e.g., Sanctuary AI, Figure, Xiaomi CyberOne) are being equipped with language models to allow natural language commanding and even autonomous decision-making in structured environments.

OpenAI invested in **1X**, a company making humanoid robots, explicitly to combine GPT-4 with a human-like form – imagine instructing a robot in plain English to do basic chores. While not public yet, it’s an active development.

On the research side, **Meta’s “Voyager”** (2023) was an agent that learned to play Minecraft (the game) using GPT-4 to generate and refine its own code (Python) to accomplish goals. It effectively wrote and debugged its behavior iteratively, achieving very complex skills in the open-ended Minecraft world ([Introducing OpenAI o3 and o4-mini | OpenAI](https://openai.com/index/introducing-o3-and-o4-mini/#:~:text=OpenAI%20o3%20is%20our%20most,for%20complex%20queries%20requiring%20multi)) ([Introducing OpenAI o3 and o4-mini | OpenAI](https://openai.com/index/introducing-o3-and-o4-mini/#:~:text=match%20at%20L152%20OpenAI%20o3,for%20complex%20queries%20requiring%20multi)). This showcases how an LLM can function as a *brain that writes its own programs for a given embodiment.* The techniques from Voyager are being considered for real robotics – where the LLM writes “skills” (small code or script) to achieve tasks and tests them, learning from failures.

**Why integrate LLMs with robots?** Because LLMs bring knowledge about the world (for example, knowing that “coffee is hot, lift the mug carefully”) and can reason abstractly, which pure robotic controllers lack. Conversely, robots provide grounding – an LLM controlling a robot gets real-world feedback (“the object is heavier than expected”), which can refine its plans.

In summary, the line between digital AI and physical robots is blurring via language models. The frontier examples – like a robot that can pick up arbitrary objects because it read about them on the internet (RT-2) ([Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen](https://qwenlm.github.io/blog/qwen2.5-max/#:~:text=Image)) ([Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model | Qwen](https://qwenlm.github.io/blog/qwen2.5-max/#:~:text=Qwen2.5,Pro)), or a surgical robot learning from videos (JHU’s work) ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=A%20robot%2C%20trained%20for%20the,skillfully%20as%20the%20human%20doctors)) ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=fundamental%20tasks%20required%20in%20surgical,as%20skillfully%20as%20human%20doctors)) – indicate that **generalist AI** is entering robotics. This could vastly accelerate automation in manufacturing, healthcare, service industries, etc., because robots will no longer be limited to narrow pre-programmed actions; they can **flexibly respond** and even learn new skills on the job using their LLM “brain” to guide trial-and-error.

### AI and Quantum Computing Crossovers

While quantum computing and AI are distinct fields, there is a growing synergy:

**Using AI for Quantum:** Quantum computers are notoriously hard to calibrate and control. In 2024, companies like Quantum Machines and Qruise demonstrated **AI-powered automated calibration** of quantum processors ([Quantum Machines and Rigetti Announce Successful AI-Powered ...](https://investors.rigetti.com/news-releases/news-release-details/quantum-machines-and-rigetti-announce-successful-ai-powered#:~:text=Quantum%20Machines%20and%20Rigetti%20Announce,with%20Quantum%20Machines%27%20control%20system)) ([Quantum Machines and Rigetti Announce Successful AI-Powered ...](https://investors.rigetti.com/news-releases/news-release-details/quantum-machines-and-rigetti-announce-successful-ai-powered#:~:text=AI,with%20Quantum%20Machines%27%20control%20system)). For example, tuning the many parameters (voltages, pulse timings) of a superconducting qubit chip is complex; an AI can treat it as an optimization problem and find the optimal settings much faster than human engineers. Rigetti Computing reported that using an AI calibration tool, they significantly shortened the time to calibrate their QPUs (Quantum Processing Units) ([Quantum Machines and Rigetti Announce Successful AI-Powered ...](https://investors.rigetti.com/news-releases/news-release-details/quantum-machines-and-rigetti-announce-successful-ai-powered#:~:text=Quantum%20Machines%20and%20Rigetti%20Announce,with%20Quantum%20Machines%27%20control%20system)) ([Quantum Machines and Rigetti Announce Successful AI-Powered ...](https://investors.rigetti.com/news-releases/news-release-details/quantum-machines-and-rigetti-announce-successful-ai-powered#:~:text=AI,with%20Quantum%20Machines%27%20control%20system)). Essentially, this is like giving a quantum computer a self-driving car’s autopilot for setup – it continuously adjusts to keep qubits stable and reduce error rates.

Another use of AI is in **quantum error correction**: some research used deep reinforcement learning to design error-correcting strategies or to dynamically adjust them as error patterns shift. By analyzing the output of quantum circuits, AI can predict where errors are likely and apply corrections proactively, improving effective qubit fidelity.

AI is also helping **quantum algorithm discovery**. Just as AlphaDev found classical algorithms, there are attempts for AI to invent new quantum algorithms or optimize quantum circuits. For instance, universities have tried using genetic algorithms or neural networks to search for shorter quantum circuits that do a desired computation, effectively compressing the circuit depth to reduce decoherence issues.

**Using Quantum for AI:** On the flip side, quantum computing promises to accelerate certain AI computations in the future (quantum machine learning). While no clear quantum advantage for practical ML has been shown yet, companies are exploring hybrid models. For example, a quantum processor could potentially solve a part of an optimization inside a classical ML (like do a kernel trick or a sampling more efficiently). 

There was mention that at least one quantum company achieved a small scale demonstration: a quantum computer doing a simple ML task slightly faster than a classical one, but these are very limited and not yet broadly useful. So as of 2025, **AI benefits quantum more than quantum benefits AI** – we use AI to enhance quantum hardware performance. 

However, the two fields are converging: organizations like IBM and Google have “Quantum AI” teams working at the intersection. **NVIDIA** even introduced cuQuantum for fast simulation of quantum circuits on GPUs, often used alongside AI models that try to learn quantum states.

One particularly interesting project: **Google’s Quantum Chemistry AI** uses deep learning to predict molecular properties (like energies) with near quantum accuracy, and the quantum computer is then used to validate or further refine those predictions for molecules where it can. This hybrid approach tackles complex chemistry with AI guess + quantum verification. 

Also, **Intel’s research 2024** listed both quantum and AI as triumphs, possibly indicating efforts to create hardware that can do both (like leveraging quantum effects in new neuromorphic chips, or using spin-qubits to implement AI computations at low power) ([Quantum and AI Projects Make List of Intel's 2024 Triumphs](https://thequantuminsider.com/2025/01/09/quantum-and-ai-projects-make-list-of-intels-2024-triumphs/#:~:text=Quantum%20and%20AI%20Projects%20Make,edge%20technologies)) ([Quantum and AI Projects Make List of Intel's 2024 Triumphs](https://thequantuminsider.com/2025/01/09/quantum-and-ai-projects-make-list-of-intels-2024-triumphs/#:~:text=Intel%20Labs%20marked%202024%20with,edge%20technologies)).

Finally, we’re seeing **conferences and teams merging** – e.g., *Quantum + AI* hackathons. D-Wave, a quantum annealer company, often pitches its tech as useful for certain machine learning optimization problems, and they highlight “quantum-fueled AI success” stories at events ([Quantum Computing Advances in 2024 Put Security In Spotlight](https://www.darkreading.com/cyber-risk/quantum-computing-advances-2024-security-spotlight#:~:text=to%20a%20workable%20quantum%20computer,seem%20closer%20than%20ever)) ([D-Wave's Qubits 2024 Quantum Computing Conference Announced ...](https://www.dwavequantum.com/company/newsroom/press-release/d-wave-s-qubits-2024-quantum-computing-conference-announced-for-june-17-18-in-boston/#:~:text=D,technology%2C%20and%20demonstrations%20of)).

In summary, while quantum computing is still emerging, AI is already making an impact by taming quantum hardware and exploring quantum algorithm spaces. If and when quantum computers become powerful, they might in turn supercharge AI processing (imagine quantum-accelerated training for certain models). Publicly, in 2024 we saw statements like *“AI-fueled quantum advancements”* and vice versa ([Quantum and AI Projects Make List of Intel's 2024 Triumphs](https://thequantuminsider.com/2025/01/09/quantum-and-ai-projects-make-list-of-intels-2024-triumphs/#:~:text=Quantum%20and%20AI%20Projects%20Make,edge%20technologies)) ([Quantum and AI Projects Make List of Intel's 2024 Triumphs](https://thequantuminsider.com/2025/01/09/quantum-and-ai-projects-make-list-of-intels-2024-triumphs/#:~:text=Intel%20Labs%20marked%202024%20with,edge%20technologies)), but it’s early days. The main tangible outcome so far is **AI reducing the overhead in quantum experiments** – a crucial step toward scalable quantum tech ([Quantum Machines and Rigetti Announce Successful AI-Powered ...](https://investors.rigetti.com/news-releases/news-release-details/quantum-machines-and-rigetti-announce-successful-ai-powered#:~:text=Quantum%20Machines%20and%20Rigetti%20Announce,with%20Quantum%20Machines%27%20control%20system)) ([Quantum Machines and Rigetti Announce Successful AI-Powered ...](https://investors.rigetti.com/news-releases/news-release-details/quantum-machines-and-rigetti-announce-successful-ai-powered#:~:text=AI,with%20Quantum%20Machines%27%20control%20system)).

---

**Conclusion:** The frontier of AI in 2024–2025 is characterized by rapid progress on all fronts: *smarter core models, more human-like audio/visual generation, deeper scientific integration, and expanded autonomy and tool use.* We now have language models that can rival human experts in specific domains, voice models that sound truly human, AI scientists proposing real discoveries, and agents that can act in the world (virtual and physical) semi-independently. These developments are being deployed in education, research, industry, and consumer applications at a breathtaking pace. While challenges remain (ensuring safety, ethics, and reliability of these powerful systems), the innovation momentum is strong. For students and professionals at Ivy League institutions and beyond, these are the technologies and capabilities to study closely – they will shape the next era of computing and societal change.

**Sources:** The information in this document is drawn from a range of up-to-date sources, including official model cards, research papers, and credible news outlets. For instance, OpenAI’s release notes and TechTarget analyses detail the features of GPT-4.5 and GPT-4o ([ChatGPT — Release Notes | OpenAI Help Center](https://help.openai.com/en/articles/6825453-chatgpt-release-notes#:~:text=Introducing%20GPT)) ([GPT-4.5 explained: Everything you need to know](https://www.techtarget.com/whatis/feature/GPT-45-explained-Everything-you-need-to-know#:~:text=GPT,math%2C%20science%20and%20logic%20problems)), Anthropic’s website outlines Claude 3.5/3.7’s abilities ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Read%20more%20,5%20Sonnet)) ([Claude 3.7 Sonnet \ Anthropic](https://www.anthropic.com/claude/sonnet#:~:text=Claude%203,computer%20use%20in%20public%20beta)), Google DeepMind’s blogs describe Gemini’s design ([
            
            Start building with Gemini 2.5 Flash
            
            
            - Google Developers Blog
            
        ](https://developers.googleblog.com/en/start-building-with-gemini-25-flash/#:~:text=match%20at%20L150%20still%20prioritizing,with%20thinking%20off%2C%20developers%20can)) ([Gemini Pro - Google DeepMind](https://deepmind.google/technologies/gemini/pro/#:~:text=match%20at%20L461%20Gemini%202,of%20benchmarks%20requiring%20enhanced%20reasoning)), VentureBeat and Lawfare provide insight on Meta’s LLaMA 4 models ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=Now%20we%20know%20the%20fruits,code%20sharing%20community%20Hugging%20Face)) ([Meta's answer to DeepSeek is here: Llama 4 launches with long context Scout and Maverick models, and 2T parameter Behemoth on the way! | VentureBeat](https://venturebeat.com/ai/metas-answer-to-deepseek-is-here-llama-4-launches-with-long-context-scout-and-maverick-models-and-2t-parameter-behemoth-on-the-way/#:~:text=One%20headline%20feature%20of%20these,audio%20was%20not%20mentioned)) and DeepSeek’s impact ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=measured%20by%20X%2C%20at%20least,a%20coding%20competition)) ([
	What DeepSeek r1 Means—and What It Doesn’t | Lawfare
](https://www.lawfaremedia.org/article/what-deepseek-r1-means-and-what-it-doesn-t#:~:text=What%E2%80%99s%20more%2C%20DeepSeek%20released%20the,apps%20like%20Gemini%20and%20Claude)), and Nature articles document AlphaFold 3’s performance ([Accurate structure prediction of biomolecular interactions with AlphaFold 3 | Nature](https://www.nature.com/articles/s41586-024-07487-w#:~:text=applications%20in%20protein%20modelling%20and,substantially%20higher%20antibody%E2%80%93antigen%20prediction%20accuracy)) ([Accurate structure prediction of biomolecular interactions with AlphaFold 3 | Nature](https://www.nature.com/articles/s41586-024-07487-w#:~:text=based%20architecture%20that%20is%20capable,space%20is%20possible%20within%20a)). For audio, Suno’s Bark GitHub and ElevenLabs releases cover voice model capabilities ([suno-ai/bark: Text-Prompted Generative Audio Model - GitHub](https://github.com/suno-ai/bark#:~:text=Bark%20is%20a%20transformer,as%20well%20as%20other%20audio)) ([Eleven Multilingual v2 | ElevenLabs](https://elevenlabs.io/blog/eleven-multilingual-v2#:~:text=Eleven%20Multilingual%20v2%20,Multilingual%20v2%20supports%2029%20languages)), while the Y Combinator launch of Sesame’s model demonstrates its realism ([Launch YC: Vogent: *Insanely* Realistic Voice Agents powered by Sesame | Y Combinator](https://www.ycombinator.com/launches/NEm-vogent-insanely-realistic-voice-agents-powered-by-sesame#:~:text=Today%2C%20we%E2%80%99re%20launching%20the%20most,out%20in%20the%20videos%20below)) ([Launch YC: Vogent: *Insanely* Realistic Voice Agents powered by Sesame | Y Combinator](https://www.ycombinator.com/launches/NEm-vogent-insanely-realistic-voice-agents-powered-by-sesame#:~:text=If%20you%E2%80%99ve%20been%20online%20recently%2C,speech%20model)). The Johns Hopkins robotics press and NVIDIA’s blog illustrate autonomous surgery achievements ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=A%20robot%2C%20trained%20for%20the,skillfully%20as%20the%20human%20doctors)) ([Robot that watched surgery videos performs with skill of human doctor | Hub](https://hub.jhu.edu/2024/11/11/surgery-robots-trained-with-videos/#:~:text=The%20team%2C%20which%20included%20Stanford,as%20skillfully%20as%20human%20doctors)), and Google’s AI blog and Drug Target Review highlight the AI co-scientist’s breakthroughs ([Google Research launches new scientific research tool, AI co-scientist](https://blog.google/feed/google-research-ai-co-scientist/#:~:text=Today%20Google%20is%20launching%20an,and%20a%20possible%20experimental%20approach)) ([Google’s AI co-scientist accelerates drug development - Drug Target Review](https://www.drugtargetreview.com/news/156914/googles-ai-co-scientist-accelerates-drug-development/#:~:text=Google%E2%80%99s%20newly%20introduced%20AI%20co,mechanisms%20relevant%20to%20drug%20discovery)). These and other cited sources throughout (indicated by 【†】) offer further reading and verification of the summarized content in each section.
